{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "from pathlib import Path\n",
    "# from typing import Optional, Tuple\n",
    "# import cv2\n",
    "# import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "# from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "import extract_utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# images_list=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/lists/images.txt\"\n",
    "# images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/images\"\n",
    "# model_name=\"dino_vits16\"\n",
    "# batch_size=1\n",
    "# output_dir=\"//home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16\"\n",
    "# which_block=-1\n",
    "# # Output\n",
    "# utils.make_output_dir(output_dir)\n",
    "# # Models\n",
    "# model_name = model_name.lower()\n",
    "# model, val_transform, patch_size, num_heads = utils.get_model(model_name)    #patch size= 16 number of heads= 6\n",
    "# # print(\"patch size=\", patch_size, \"number of heads=\", num_heads)\n",
    "# # Add hook\n",
    "# if 'dino' in model_name or 'mocov3' in model_name:\n",
    "#     feat_out = {}\n",
    "#     def hook_fn_forward_qkv(module, input, output):\n",
    "#         # print(\"feat_out.keys()\", feat_out.keys())\n",
    "#         feat_out[\"qkv\"] = output\n",
    "#     model._modules[\"blocks\"][which_block]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "# else:\n",
    "#     raise ValueError(model_name)\n",
    "# # Dataset\n",
    "# filenames = Path(images_list).read_text().splitlines()\n",
    "# dataset = utils.ImagesDataset(filenames=filenames, images_root=images_root, transform=val_transform)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8)\n",
    "# # print(f'Dataset size: {len(dataset)=}')\n",
    "# # print(f'Dataloader size: {len(dataloader)=}')\n",
    "# # Prepare\n",
    "# # accelerator = Accelerator(fp16=True, cpu=False)\n",
    "# accelerator = Accelerator(cpu=False)\n",
    "# # model, dataloader = accelerator.prepare(model, dataloader)\n",
    "# model = model.to(accelerator.device)\n",
    "# model.num_features\n",
    "# # print(\"model.num_features=\", model.num_features)\n",
    "# # model.get_submodule\n",
    "# # Process\n",
    "# pbar = list(tqdm(dataloader, desc='Processing'))\n",
    "# # Process\n",
    "# # pbar = list(tqdm(dataloader, desc='Processing'))\n",
    "# # print(\"feat_out.keys()=\", feat_out.keys())\n",
    "# for i, (images, files, indices) in enumerate(pbar):\n",
    "#     output_dict = {}\n",
    "#     # print(\"images.shape=\", images.shape, \"files =\", files, \"indices\", indices)\n",
    "#\n",
    "#     # Check if file already exists\n",
    "#     id = Path(files[0]).stem\n",
    "#     # print(\"id=\", id)\n",
    "#     output_file = Path(output_dir) / f'{id}.pth'\n",
    "#     # if output_file.is_file():\n",
    "#     #     pbar.write(f'Skipping existing file {str(output_file)}')\n",
    "#     #     continue\n",
    "#\n",
    "#     # Reshape image\n",
    "#     P = patch_size\n",
    "#     B, C, H, W = images.shape\n",
    "#     H_patch, W_patch = H // P, W // P\n",
    "#     H_pad, W_pad = H_patch * P, W_patch * P\n",
    "#     T = H_patch * W_patch + 1  # number of tokens, add 1 for [CLS]\n",
    "#     # images = F.interpolate(images, size=(H_pad, W_pad), mode='bilinear')  # resize image\n",
    "#     images = images[:, :, :H_pad, :W_pad]\n",
    "#     images = images.to(accelerator.device)\n",
    "#     # print(\"images.shape after padding=\", images.shape)\n",
    "#\n",
    "#     # Forward and collect features into output dict\n",
    "#     if 'dino' in model_name or 'mocov3' in model_name:\n",
    "#         # accelerator.unwrap_model(model).get_intermediate_layers(images)[0].squeeze(0)\n",
    "#         model.get_intermediate_layers(images)[0].squeeze(0)\n",
    "#         # print(model.get_intermediate_layers(images)[0])\n",
    "#         # output_dict['out'] = out\n",
    "#         # print(\"feat_out.keys()=\", feat_out.keys())\n",
    "#         output_qkv = feat_out[\"qkv\"].reshape(B, T, 3, num_heads, -1 // num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         # print(\"type(output_qkv)\", type(output_qkv))\n",
    "#         # print(\"output_qkv.shape\", output_qkv.shape)    #3, 1, 6, 931, 64]\n",
    "#         # print(\"output_qkv[0].shape\", output_qkv[0].shape)\n",
    "#         # print(\"output_qkv[1].shape\", output_qkv[1].shape)\n",
    "#         # print(\"output_qkv[2].shape\", output_qkv[2].shape)\n",
    "#         # output_dict['q'] = output_qkv[0].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#         output_dict['k'] = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#         # print(\"output_dict[k].shape=\", output_dict['k'].shape)\n",
    "#         # output_dict['v'] = output_qkv[2].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#     else:\n",
    "#         raise ValueError(model_name)\n",
    "#\n",
    "#     # print(\"output_dict.items=\", output_dict.items())\n",
    "#     # Metadata\n",
    "#     output_dict['indices'] = indices[0]\n",
    "#     output_dict['file'] = files[0]\n",
    "#     output_dict['id'] = id\n",
    "#     output_dict['model_name'] = model_name\n",
    "#     output_dict['patch_size'] = patch_size\n",
    "#     output_dict['shape'] = (B, C, H, W)\n",
    "#     output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "#     # for k, v in output_dict.items():\n",
    "#     #     print(\"k=\", k)\n",
    "#     #     if torch.is_tensor(v):\n",
    "#     #         print(\"v.shape\", v.shape)\n",
    "#     #     else:\n",
    "#     #         print(\"v=\", v)\n",
    "#     # print(\"output_dict.keys()\", output_dict.keys())\n",
    "#     # print(\"output_dict['k'].shape=\", output_dict['k'].shape,\"output_dict['indices'] =\", indices[0],\"output_dict['file'] =\", files[0],\"output_dict['id']=\" , id, \"output_dict['model_name'] =\", model_name,\" output_dict['shape'] =(\", B, C, H, W,\")output_dict['patch_size'] =\",  patch_size )\n",
    "#     # Save\n",
    "#     accelerator.save(output_dict, str(output_file))\n",
    "#     accelerator.wait_for_everyone()\n",
    "# print(f'Saved features to {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "# print(output_dict.keys())\n",
    "# for k, v in output_dict.items():\n",
    "#     print(\"k=\", k)\n",
    "#     print(\"v.shape=\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/images\"\n",
    "features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16\"\n",
    "output_dir=\"//home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/eigs_residualsimsiam_ds_100_jn\"\n",
    "which_matrix= 'laplacian'\n",
    "which_color_matrix= 'knn'\n",
    "which_features= 'k'\n",
    "normalize=True\n",
    "threshold_at_zero=True\n",
    "lapnorm= True\n",
    "K= 5\n",
    "image_downsample_factor = None\n",
    "image_color_lambda = 0.0\n",
    "multiprocessing = 0\n",
    "batch_size=2\n",
    "epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Residual Block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(128, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        # print(\"Before squeezing, out shape=\", out.shape)\n",
    "        out =  out.squeeze().to('cuda')\n",
    "        # print(\"After squeezing, out shape=\", out.shape)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         # shortcut connection\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=bias),\n",
    "#                 nn.BatchNorm1d(out_channels)\n",
    "#             )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#\n",
    "#         out += self.shortcut(residual)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# class ResNetBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "#         super(ResNetBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         self.downsample = downsample\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "#\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incorporating SimSiam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Feature_Dataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# print(feats.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.projection_head = SimSiamProjectionHead(feats.shape[1], 128,feats.shape[1])\n",
    "        self.projection_head = BasicBlock()\n",
    "        # self.projection_head = ResidualBlock(feats.shape[1], feats.shape[1])\n",
    "        self.prediction_head = SimSiamPredictionHead(feats.shape[1], 128, feats.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.projection_head(x)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# utils.make_output_dir(output_dir)\n",
    "# feat_list=[]\n",
    "# inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "# batch_size=2\n",
    "# token_feature_size=384\n",
    "# for inp in tqdm(inputs[:2]):\n",
    "#     index, features_file = inp\n",
    "#     print(index, features_file)\n",
    "#      # Load\n",
    "#     data_dict = torch.load(features_file, map_location='cpu')\n",
    "#     # Load affinity matrix\n",
    "#     # feats_unsqueeze=data_dict[which_features].cuda()\n",
    "#     feats = data_dict[which_features].squeeze().cuda()\n",
    "#     print(feats.shape)\n",
    "#     # print(feats_unsqueeze.shape)\n",
    "#     feat_dataset = Feature_Dataset(feats)\n",
    "#     features_dataloader = DataLoader(feat_dataset, batch_size=2, shuffle=True)\n",
    "#     i=0\n",
    "#     for x0 in features_dataloader:\n",
    "#         if i==0:\n",
    "#         # for (x0), _, _ in features_dataloader:\n",
    "#             print(\"before unsqueezing x0.shape=\", x0.shape)\n",
    "#             x0 = x0.unsqueeze(0).to(device)\n",
    "#             x1=torchvision.transforms.RandomAffine(0)(x0)\n",
    "#             x0 = x0.squeeze(0).to(device)\n",
    "#             x0 = torch.tensor(x0).view(batch_size, 1, 1, token_feature_size)\n",
    "#             print(\"After squeezing and viewing x0 shape=\", x0.shape)\n",
    "#             # print(\"x0.shape=\", x0.shape)\n",
    "#             x1 = x1.squeeze(0).to(device)\n",
    "#             x1 = torch.tensor(x1).view(batch_size, 1, 1,token_feature_size)\n",
    "#             print(\"After squeezing and viewing x0 shape=\", x1.shape)\n",
    "#             print(\"x1 shape=\", x1.shape)\n",
    "#         else:\n",
    "#             break\n",
    "#         i+=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0001.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0001\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.44829\n",
      "epoch: 01, loss: -0.61703\n",
      "epoch: 02, loss: -0.63711\n",
      "epoch: 03, loss: -0.63721\n",
      "epoch: 04, loss: -0.65594\n",
      "epoch: 05, loss: -0.67486\n",
      "epoch: 06, loss: -0.69277\n",
      "epoch: 07, loss: -0.70015\n",
      "epoch: 08, loss: -0.70413\n",
      "epoch: 09, loss: -0.71062\n",
      "epoch: 10, loss: -0.71754\n",
      "epoch: 11, loss: -0.69381\n",
      "epoch: 12, loss: -0.69155\n",
      "epoch: 13, loss: -0.70303\n",
      "epoch: 14, loss: -0.69959\n",
      "epoch: 15, loss: -0.70248\n",
      "epoch: 16, loss: -0.71048\n",
      "epoch: 17, loss: -0.72891\n",
      "epoch: 18, loss: -0.74677\n",
      "epoch: 19, loss: -0.74242\n",
      "epoch: 20, loss: -0.74286\n",
      "epoch: 21, loss: -0.74395\n",
      "epoch: 22, loss: -0.73876\n",
      "epoch: 23, loss: -0.74290\n",
      "epoch: 24, loss: -0.74756\n",
      "epoch: 25, loss: -0.76113\n",
      "epoch: 26, loss: -0.76022\n",
      "epoch: 27, loss: -0.76453\n",
      "epoch: 28, loss: -0.77091\n",
      "epoch: 29, loss: -0.77668\n",
      "epoch: 30, loss: -0.76109\n",
      "epoch: 31, loss: -0.74224\n",
      "epoch: 32, loss: -0.74277\n",
      "epoch: 33, loss: -0.74661\n",
      "epoch: 34, loss: -0.75799\n",
      "epoch: 35, loss: -0.75088\n",
      "epoch: 36, loss: -0.75882\n",
      "epoch: 37, loss: -0.77441\n",
      "epoch: 38, loss: -0.78740\n",
      "epoch: 39, loss: -0.80005\n",
      "epoch: 40, loss: -0.78362\n",
      "epoch: 41, loss: -0.79114\n",
      "epoch: 42, loss: -0.78410\n",
      "epoch: 43, loss: -0.78661\n",
      "epoch: 44, loss: -0.80484\n",
      "epoch: 45, loss: -0.82089\n",
      "epoch: 46, loss: -0.80690\n",
      "epoch: 47, loss: -0.79308\n",
      "epoch: 48, loss: -0.80159\n",
      "epoch: 49, loss: -0.79882\n",
      "epoch: 50, loss: -0.79174\n",
      "epoch: 51, loss: -0.77882\n",
      "epoch: 52, loss: -0.77462\n",
      "epoch: 53, loss: -0.76625\n",
      "epoch: 54, loss: -0.74087\n",
      "epoch: 55, loss: -0.73717\n",
      "epoch: 56, loss: -0.75387\n",
      "epoch: 57, loss: -0.76276\n",
      "epoch: 58, loss: -0.79408\n",
      "epoch: 59, loss: -0.80609\n",
      "epoch: 60, loss: -0.81669\n",
      "epoch: 61, loss: -0.81389\n",
      "epoch: 62, loss: -0.82105\n",
      "epoch: 63, loss: -0.81944\n",
      "epoch: 64, loss: -0.83655\n",
      "epoch: 65, loss: -0.84496\n",
      "epoch: 66, loss: -0.85976\n",
      "epoch: 67, loss: -0.85764\n",
      "epoch: 68, loss: -0.85071\n",
      "epoch: 69, loss: -0.85058\n",
      "epoch: 70, loss: -0.84935\n",
      "epoch: 71, loss: -0.84738\n",
      "epoch: 72, loss: -0.84588\n",
      "epoch: 73, loss: -0.85716\n",
      "epoch: 74, loss: -0.85062\n",
      "epoch: 75, loss: -0.85351\n",
      "epoch: 76, loss: -0.84589\n",
      "epoch: 77, loss: -0.84228\n",
      "epoch: 78, loss: -0.83901\n",
      "epoch: 79, loss: -0.85268\n",
      "epoch: 80, loss: -0.84708\n",
      "epoch: 81, loss: -0.85639\n",
      "epoch: 82, loss: -0.86059\n",
      "epoch: 83, loss: -0.86435\n",
      "epoch: 84, loss: -0.87773\n",
      "epoch: 85, loss: -0.87102\n",
      "epoch: 86, loss: -0.86557\n",
      "epoch: 87, loss: -0.86371\n",
      "epoch: 88, loss: -0.86152\n",
      "epoch: 89, loss: -0.86826\n",
      "epoch: 90, loss: -0.86859\n",
      "epoch: 91, loss: -0.87597\n",
      "epoch: 92, loss: -0.87971\n",
      "epoch: 93, loss: -0.88688\n",
      "epoch: 94, loss: -0.89074\n",
      "epoch: 95, loss: -0.89814\n",
      "epoch: 96, loss: -0.90176\n",
      "epoch: 97, loss: -0.90778\n",
      "epoch: 98, loss: -0.91032\n",
      "epoch: 99, loss: -0.91115\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [02:25<40:14:28, 145.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "1 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0002.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0002\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.52578\n",
      "epoch: 01, loss: -0.65261\n",
      "epoch: 02, loss: -0.69198\n",
      "epoch: 03, loss: -0.70648\n",
      "epoch: 04, loss: -0.73690\n",
      "epoch: 05, loss: -0.75099\n",
      "epoch: 06, loss: -0.74505\n",
      "epoch: 07, loss: -0.74420\n",
      "epoch: 08, loss: -0.74976\n",
      "epoch: 09, loss: -0.75214\n",
      "epoch: 10, loss: -0.74839\n",
      "epoch: 11, loss: -0.74499\n",
      "epoch: 12, loss: -0.76017\n",
      "epoch: 13, loss: -0.76023\n",
      "epoch: 14, loss: -0.77265\n",
      "epoch: 15, loss: -0.77368\n",
      "epoch: 16, loss: -0.76745\n",
      "epoch: 17, loss: -0.77121\n",
      "epoch: 18, loss: -0.77104\n",
      "epoch: 19, loss: -0.76646\n",
      "epoch: 20, loss: -0.78176\n",
      "epoch: 21, loss: -0.79074\n",
      "epoch: 22, loss: -0.81268\n",
      "epoch: 23, loss: -0.82378\n",
      "epoch: 24, loss: -0.84539\n",
      "epoch: 25, loss: -0.85754\n",
      "epoch: 26, loss: -0.85300\n",
      "epoch: 27, loss: -0.85067\n",
      "epoch: 28, loss: -0.83528\n",
      "epoch: 29, loss: -0.84139\n",
      "epoch: 30, loss: -0.86038\n",
      "epoch: 31, loss: -0.86161\n",
      "epoch: 32, loss: -0.86285\n",
      "epoch: 33, loss: -0.86485\n",
      "epoch: 34, loss: -0.86058\n",
      "epoch: 35, loss: -0.86381\n",
      "epoch: 36, loss: -0.86542\n",
      "epoch: 37, loss: -0.85243\n",
      "epoch: 38, loss: -0.84763\n",
      "epoch: 39, loss: -0.85568\n",
      "epoch: 40, loss: -0.86237\n",
      "epoch: 41, loss: -0.86513\n",
      "epoch: 42, loss: -0.86004\n",
      "epoch: 43, loss: -0.86534\n",
      "epoch: 44, loss: -0.85774\n",
      "epoch: 45, loss: -0.85951\n",
      "epoch: 46, loss: -0.87672\n",
      "epoch: 47, loss: -0.86429\n",
      "epoch: 48, loss: -0.86025\n",
      "epoch: 49, loss: -0.85555\n",
      "epoch: 50, loss: -0.85433\n",
      "epoch: 51, loss: -0.85711\n",
      "epoch: 52, loss: -0.85871\n",
      "epoch: 53, loss: -0.86068\n",
      "epoch: 54, loss: -0.85370\n",
      "epoch: 55, loss: -0.85223\n",
      "epoch: 56, loss: -0.85072\n",
      "epoch: 57, loss: -0.85305\n",
      "epoch: 58, loss: -0.85391\n",
      "epoch: 59, loss: -0.85303\n",
      "epoch: 60, loss: -0.85526\n",
      "epoch: 61, loss: -0.86423\n",
      "epoch: 62, loss: -0.86829\n",
      "epoch: 63, loss: -0.87522\n",
      "epoch: 64, loss: -0.88868\n",
      "epoch: 65, loss: -0.88980\n",
      "epoch: 66, loss: -0.88913\n",
      "epoch: 67, loss: -0.88610\n",
      "epoch: 68, loss: -0.87814\n",
      "epoch: 69, loss: -0.88254\n",
      "epoch: 70, loss: -0.87526\n",
      "epoch: 71, loss: -0.86654\n",
      "epoch: 72, loss: -0.87016\n",
      "epoch: 73, loss: -0.87568\n",
      "epoch: 74, loss: -0.87937\n",
      "epoch: 75, loss: -0.88756\n",
      "epoch: 76, loss: -0.89974\n",
      "epoch: 77, loss: -0.91030\n",
      "epoch: 78, loss: -0.90383\n",
      "epoch: 79, loss: -0.91020\n",
      "epoch: 80, loss: -0.92279\n",
      "epoch: 81, loss: -0.92562\n",
      "epoch: 82, loss: -0.92613\n",
      "epoch: 83, loss: -0.92118\n",
      "epoch: 84, loss: -0.92295\n",
      "epoch: 85, loss: -0.92294\n",
      "epoch: 86, loss: -0.92799\n",
      "epoch: 87, loss: -0.93120\n",
      "epoch: 88, loss: -0.93061\n",
      "epoch: 89, loss: -0.93499\n",
      "epoch: 90, loss: -0.93232\n",
      "epoch: 91, loss: -0.94028\n",
      "epoch: 92, loss: -0.94479\n",
      "epoch: 93, loss: -0.94507\n",
      "epoch: 94, loss: -0.94399\n",
      "epoch: 95, loss: -0.93917\n",
      "epoch: 96, loss: -0.93433\n",
      "epoch: 97, loss: -0.93328\n",
      "epoch: 98, loss: -0.92564\n",
      "epoch: 99, loss: -0.92355\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [04:48<39:59:07, 144.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "2 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0003.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0003\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51048\n",
      "epoch: 01, loss: -0.65143\n",
      "epoch: 02, loss: -0.67221\n",
      "epoch: 03, loss: -0.68985\n",
      "epoch: 04, loss: -0.71226\n",
      "epoch: 05, loss: -0.73600\n",
      "epoch: 06, loss: -0.73184\n",
      "epoch: 07, loss: -0.73328\n",
      "epoch: 08, loss: -0.73433\n",
      "epoch: 09, loss: -0.74077\n",
      "epoch: 10, loss: -0.75231\n",
      "epoch: 11, loss: -0.76275\n",
      "epoch: 12, loss: -0.75813\n",
      "epoch: 13, loss: -0.76517\n",
      "epoch: 14, loss: -0.76110\n",
      "epoch: 15, loss: -0.77471\n",
      "epoch: 16, loss: -0.79814\n",
      "epoch: 17, loss: -0.79233\n",
      "epoch: 18, loss: -0.78724\n",
      "epoch: 19, loss: -0.79263\n",
      "epoch: 20, loss: -0.79031\n",
      "epoch: 21, loss: -0.77845\n",
      "epoch: 22, loss: -0.77114\n",
      "epoch: 23, loss: -0.77607\n",
      "epoch: 24, loss: -0.76988\n",
      "epoch: 25, loss: -0.79085\n",
      "epoch: 26, loss: -0.79130\n",
      "epoch: 27, loss: -0.81244\n",
      "epoch: 28, loss: -0.80262\n",
      "epoch: 29, loss: -0.80990\n",
      "epoch: 30, loss: -0.80496\n",
      "epoch: 31, loss: -0.80436\n",
      "epoch: 32, loss: -0.79948\n",
      "epoch: 33, loss: -0.80138\n",
      "epoch: 34, loss: -0.79017\n",
      "epoch: 35, loss: -0.79797\n",
      "epoch: 36, loss: -0.78984\n",
      "epoch: 37, loss: -0.79205\n",
      "epoch: 38, loss: -0.78696\n",
      "epoch: 39, loss: -0.79248\n",
      "epoch: 40, loss: -0.79471\n",
      "epoch: 41, loss: -0.79193\n",
      "epoch: 42, loss: -0.79021\n",
      "epoch: 43, loss: -0.80347\n",
      "epoch: 44, loss: -0.79587\n",
      "epoch: 45, loss: -0.79315\n",
      "epoch: 46, loss: -0.80464\n",
      "epoch: 47, loss: -0.80969\n",
      "epoch: 48, loss: -0.81722\n",
      "epoch: 49, loss: -0.81453\n",
      "epoch: 50, loss: -0.81625\n",
      "epoch: 51, loss: -0.81012\n",
      "epoch: 52, loss: -0.80070\n",
      "epoch: 53, loss: -0.78902\n",
      "epoch: 54, loss: -0.79409\n",
      "epoch: 55, loss: -0.78422\n",
      "epoch: 56, loss: -0.78362\n",
      "epoch: 57, loss: -0.79914\n",
      "epoch: 58, loss: -0.79552\n",
      "epoch: 59, loss: -0.81026\n",
      "epoch: 60, loss: -0.81977\n",
      "epoch: 61, loss: -0.81506\n",
      "epoch: 62, loss: -0.81187\n",
      "epoch: 63, loss: -0.81463\n",
      "epoch: 64, loss: -0.81065\n",
      "epoch: 65, loss: -0.79507\n",
      "epoch: 66, loss: -0.80292\n",
      "epoch: 67, loss: -0.80183\n",
      "epoch: 68, loss: -0.81416\n",
      "epoch: 69, loss: -0.80131\n",
      "epoch: 70, loss: -0.79611\n",
      "epoch: 71, loss: -0.80112\n",
      "epoch: 72, loss: -0.80532\n",
      "epoch: 73, loss: -0.80208\n",
      "epoch: 74, loss: -0.80092\n",
      "epoch: 75, loss: -0.79829\n",
      "epoch: 76, loss: -0.80450\n",
      "epoch: 77, loss: -0.80233\n",
      "epoch: 78, loss: -0.79865\n",
      "epoch: 79, loss: -0.79955\n",
      "epoch: 80, loss: -0.80283\n",
      "epoch: 81, loss: -0.80953\n",
      "epoch: 82, loss: -0.82100\n",
      "epoch: 83, loss: -0.82118\n",
      "epoch: 84, loss: -0.81931\n",
      "epoch: 85, loss: -0.81076\n",
      "epoch: 86, loss: -0.80675\n",
      "epoch: 87, loss: -0.81853\n",
      "epoch: 88, loss: -0.81754\n",
      "epoch: 89, loss: -0.82339\n",
      "epoch: 90, loss: -0.82260\n",
      "epoch: 91, loss: -0.82197\n",
      "epoch: 92, loss: -0.82080\n",
      "epoch: 93, loss: -0.82715\n",
      "epoch: 94, loss: -0.82834\n",
      "epoch: 95, loss: -0.83317\n",
      "epoch: 96, loss: -0.82206\n",
      "epoch: 97, loss: -0.83277\n",
      "epoch: 98, loss: -0.83085\n",
      "epoch: 99, loss: -0.83300\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [07:23<41:19:35, 149.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "3 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0004.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0004\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51926\n",
      "epoch: 01, loss: -0.65674\n",
      "epoch: 02, loss: -0.68598\n",
      "epoch: 03, loss: -0.68986\n",
      "epoch: 04, loss: -0.71733\n",
      "epoch: 05, loss: -0.71452\n",
      "epoch: 06, loss: -0.72818\n",
      "epoch: 07, loss: -0.73532\n",
      "epoch: 08, loss: -0.75041\n",
      "epoch: 09, loss: -0.76671\n",
      "epoch: 10, loss: -0.79385\n",
      "epoch: 11, loss: -0.79319\n",
      "epoch: 12, loss: -0.79770\n",
      "epoch: 13, loss: -0.80813\n",
      "epoch: 14, loss: -0.80942\n",
      "epoch: 15, loss: -0.80177\n",
      "epoch: 16, loss: -0.81765\n",
      "epoch: 17, loss: -0.82362\n",
      "epoch: 18, loss: -0.83227\n",
      "epoch: 19, loss: -0.81697\n",
      "epoch: 20, loss: -0.81227\n",
      "epoch: 21, loss: -0.82136\n",
      "epoch: 22, loss: -0.82458\n",
      "epoch: 23, loss: -0.83265\n",
      "epoch: 24, loss: -0.83434\n",
      "epoch: 25, loss: -0.84548\n",
      "epoch: 26, loss: -0.85678\n",
      "epoch: 27, loss: -0.85432\n",
      "epoch: 28, loss: -0.85225\n",
      "epoch: 29, loss: -0.86084\n",
      "epoch: 30, loss: -0.86078\n",
      "epoch: 31, loss: -0.85069\n",
      "epoch: 32, loss: -0.84920\n",
      "epoch: 33, loss: -0.84862\n",
      "epoch: 34, loss: -0.84848\n",
      "epoch: 35, loss: -0.84769\n",
      "epoch: 36, loss: -0.83659\n",
      "epoch: 37, loss: -0.83387\n",
      "epoch: 38, loss: -0.83349\n",
      "epoch: 39, loss: -0.81995\n",
      "epoch: 40, loss: -0.81382\n",
      "epoch: 41, loss: -0.82404\n",
      "epoch: 42, loss: -0.82839\n",
      "epoch: 43, loss: -0.83128\n",
      "epoch: 44, loss: -0.82408\n",
      "epoch: 45, loss: -0.82684\n",
      "epoch: 46, loss: -0.82078\n",
      "epoch: 47, loss: -0.82479\n",
      "epoch: 48, loss: -0.82054\n",
      "epoch: 49, loss: -0.82198\n",
      "epoch: 50, loss: -0.82087\n",
      "epoch: 51, loss: -0.82144\n",
      "epoch: 52, loss: -0.80912\n",
      "epoch: 53, loss: -0.81673\n",
      "epoch: 54, loss: -0.81942\n",
      "epoch: 55, loss: -0.82297\n",
      "epoch: 56, loss: -0.82758\n",
      "epoch: 57, loss: -0.82760\n",
      "epoch: 58, loss: -0.82933\n",
      "epoch: 59, loss: -0.83018\n",
      "epoch: 60, loss: -0.82427\n",
      "epoch: 61, loss: -0.84166\n",
      "epoch: 62, loss: -0.85071\n",
      "epoch: 63, loss: -0.84758\n",
      "epoch: 64, loss: -0.84757\n",
      "epoch: 65, loss: -0.85438\n",
      "epoch: 66, loss: -0.86031\n",
      "epoch: 67, loss: -0.86108\n",
      "epoch: 68, loss: -0.86676\n",
      "epoch: 69, loss: -0.86478\n",
      "epoch: 70, loss: -0.85146\n",
      "epoch: 71, loss: -0.84118\n",
      "epoch: 72, loss: -0.83826\n",
      "epoch: 73, loss: -0.84042\n",
      "epoch: 74, loss: -0.83782\n",
      "epoch: 75, loss: -0.84599\n",
      "epoch: 76, loss: -0.84702\n",
      "epoch: 77, loss: -0.84667\n",
      "epoch: 78, loss: -0.84163\n",
      "epoch: 79, loss: -0.84530\n",
      "epoch: 80, loss: -0.84362\n",
      "epoch: 81, loss: -0.85258\n",
      "epoch: 82, loss: -0.85996\n",
      "epoch: 83, loss: -0.86499\n",
      "epoch: 84, loss: -0.86168\n",
      "epoch: 85, loss: -0.86405\n",
      "epoch: 86, loss: -0.86742\n",
      "epoch: 87, loss: -0.87322\n",
      "epoch: 88, loss: -0.86554\n",
      "epoch: 89, loss: -0.86502\n",
      "epoch: 90, loss: -0.87186\n",
      "epoch: 91, loss: -0.86214\n",
      "epoch: 92, loss: -0.86180\n",
      "epoch: 93, loss: -0.86170\n",
      "epoch: 94, loss: -0.86298\n",
      "epoch: 95, loss: -0.86578\n",
      "epoch: 96, loss: -0.87217\n",
      "epoch: 97, loss: -0.87284\n",
      "epoch: 98, loss: -0.87677\n",
      "epoch: 99, loss: -0.88191\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [10:02<42:18:37, 152.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "4 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0005.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0005\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54029\n",
      "epoch: 01, loss: -0.67239\n",
      "epoch: 02, loss: -0.70613\n",
      "epoch: 03, loss: -0.69548\n",
      "epoch: 04, loss: -0.70249\n",
      "epoch: 05, loss: -0.70552\n",
      "epoch: 06, loss: -0.71595\n",
      "epoch: 07, loss: -0.71611\n",
      "epoch: 08, loss: -0.72163\n",
      "epoch: 09, loss: -0.72780\n",
      "epoch: 10, loss: -0.73089\n",
      "epoch: 11, loss: -0.72871\n",
      "epoch: 12, loss: -0.73396\n",
      "epoch: 13, loss: -0.74926\n",
      "epoch: 14, loss: -0.76522\n",
      "epoch: 15, loss: -0.76204\n",
      "epoch: 16, loss: -0.77270\n",
      "epoch: 17, loss: -0.77106\n",
      "epoch: 18, loss: -0.76715\n",
      "epoch: 19, loss: -0.78070\n",
      "epoch: 20, loss: -0.77537\n",
      "epoch: 21, loss: -0.77141\n",
      "epoch: 22, loss: -0.77737\n",
      "epoch: 23, loss: -0.77446\n",
      "epoch: 24, loss: -0.78005\n",
      "epoch: 25, loss: -0.78609\n",
      "epoch: 26, loss: -0.78652\n",
      "epoch: 27, loss: -0.79347\n",
      "epoch: 28, loss: -0.79698\n",
      "epoch: 29, loss: -0.80663\n",
      "epoch: 30, loss: -0.81299\n",
      "epoch: 31, loss: -0.81391\n",
      "epoch: 32, loss: -0.81652\n",
      "epoch: 33, loss: -0.81728\n",
      "epoch: 34, loss: -0.82281\n",
      "epoch: 35, loss: -0.82644\n",
      "epoch: 36, loss: -0.83229\n",
      "epoch: 37, loss: -0.83169\n",
      "epoch: 38, loss: -0.83590\n",
      "epoch: 39, loss: -0.84776\n",
      "epoch: 40, loss: -0.84551\n",
      "epoch: 41, loss: -0.84735\n",
      "epoch: 42, loss: -0.84152\n",
      "epoch: 43, loss: -0.83138\n",
      "epoch: 44, loss: -0.82790\n",
      "epoch: 45, loss: -0.82396\n",
      "epoch: 46, loss: -0.81060\n",
      "epoch: 47, loss: -0.81044\n",
      "epoch: 48, loss: -0.81356\n",
      "epoch: 49, loss: -0.81244\n",
      "epoch: 50, loss: -0.81495\n",
      "epoch: 51, loss: -0.82830\n",
      "epoch: 52, loss: -0.82510\n",
      "epoch: 53, loss: -0.83019\n",
      "epoch: 54, loss: -0.82649\n",
      "epoch: 55, loss: -0.81567\n",
      "epoch: 56, loss: -0.82031\n",
      "epoch: 57, loss: -0.81263\n",
      "epoch: 58, loss: -0.82119\n",
      "epoch: 59, loss: -0.82056\n",
      "epoch: 60, loss: -0.81485\n",
      "epoch: 61, loss: -0.81896\n",
      "epoch: 62, loss: -0.83004\n",
      "epoch: 63, loss: -0.83112\n",
      "epoch: 64, loss: -0.83744\n",
      "epoch: 65, loss: -0.85734\n",
      "epoch: 66, loss: -0.86803\n",
      "epoch: 67, loss: -0.88170\n",
      "epoch: 68, loss: -0.88182\n",
      "epoch: 69, loss: -0.88983\n",
      "epoch: 70, loss: -0.89214\n",
      "epoch: 71, loss: -0.87945\n",
      "epoch: 72, loss: -0.87721\n",
      "epoch: 73, loss: -0.88333\n",
      "epoch: 74, loss: -0.88796\n",
      "epoch: 75, loss: -0.89360\n",
      "epoch: 76, loss: -0.90049\n",
      "epoch: 77, loss: -0.89742\n",
      "epoch: 78, loss: -0.90685\n",
      "epoch: 79, loss: -0.90884\n",
      "epoch: 80, loss: -0.90771\n",
      "epoch: 81, loss: -0.90872\n",
      "epoch: 82, loss: -0.91141\n",
      "epoch: 83, loss: -0.90943\n",
      "epoch: 84, loss: -0.90660\n",
      "epoch: 85, loss: -0.90818\n",
      "epoch: 86, loss: -0.90818\n",
      "epoch: 87, loss: -0.91395\n",
      "epoch: 88, loss: -0.91277\n",
      "epoch: 89, loss: -0.90710\n",
      "epoch: 90, loss: -0.90860\n",
      "epoch: 91, loss: -0.90969\n",
      "epoch: 92, loss: -0.90382\n",
      "epoch: 93, loss: -0.89835\n",
      "epoch: 94, loss: -0.90212\n",
      "epoch: 95, loss: -0.89394\n",
      "epoch: 96, loss: -0.89681\n",
      "epoch: 97, loss: -0.90095\n",
      "epoch: 98, loss: -0.90481\n",
      "epoch: 99, loss: -0.90861\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [12:43<43:05:02, 155.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "5 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0006.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0006\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.46822\n",
      "epoch: 01, loss: -0.60073\n",
      "epoch: 02, loss: -0.62614\n",
      "epoch: 03, loss: -0.64261\n",
      "epoch: 04, loss: -0.65195\n",
      "epoch: 05, loss: -0.66138\n",
      "epoch: 06, loss: -0.67684\n",
      "epoch: 07, loss: -0.67421\n",
      "epoch: 08, loss: -0.68113\n",
      "epoch: 09, loss: -0.68184\n",
      "epoch: 10, loss: -0.69620\n",
      "epoch: 11, loss: -0.69377\n",
      "epoch: 12, loss: -0.70214\n",
      "epoch: 13, loss: -0.69841\n",
      "epoch: 14, loss: -0.68972\n",
      "epoch: 15, loss: -0.67228\n",
      "epoch: 16, loss: -0.66542\n",
      "epoch: 17, loss: -0.67720\n",
      "epoch: 18, loss: -0.67927\n",
      "epoch: 19, loss: -0.68220\n",
      "epoch: 20, loss: -0.68665\n",
      "epoch: 21, loss: -0.70895\n",
      "epoch: 22, loss: -0.71903\n",
      "epoch: 23, loss: -0.71335\n",
      "epoch: 24, loss: -0.71802\n",
      "epoch: 25, loss: -0.71756\n",
      "epoch: 26, loss: -0.70206\n",
      "epoch: 27, loss: -0.71038\n",
      "epoch: 28, loss: -0.71540\n",
      "epoch: 29, loss: -0.71392\n",
      "epoch: 30, loss: -0.72264\n",
      "epoch: 31, loss: -0.72718\n",
      "epoch: 32, loss: -0.72733\n",
      "epoch: 33, loss: -0.71772\n",
      "epoch: 34, loss: -0.72299\n",
      "epoch: 35, loss: -0.70881\n",
      "epoch: 36, loss: -0.70598\n",
      "epoch: 37, loss: -0.70872\n",
      "epoch: 38, loss: -0.70034\n",
      "epoch: 39, loss: -0.70136\n",
      "epoch: 40, loss: -0.70912\n",
      "epoch: 41, loss: -0.71291\n",
      "epoch: 42, loss: -0.70647\n",
      "epoch: 43, loss: -0.69304\n",
      "epoch: 44, loss: -0.69079\n",
      "epoch: 45, loss: -0.70078\n",
      "epoch: 46, loss: -0.70142\n",
      "epoch: 47, loss: -0.70203\n",
      "epoch: 48, loss: -0.72892\n",
      "epoch: 49, loss: -0.73344\n",
      "epoch: 50, loss: -0.73891\n",
      "epoch: 51, loss: -0.73682\n",
      "epoch: 52, loss: -0.73246\n",
      "epoch: 53, loss: -0.72444\n",
      "epoch: 54, loss: -0.73481\n",
      "epoch: 55, loss: -0.74075\n",
      "epoch: 56, loss: -0.73455\n",
      "epoch: 57, loss: -0.71781\n",
      "epoch: 58, loss: -0.71543\n",
      "epoch: 59, loss: -0.73057\n",
      "epoch: 60, loss: -0.72370\n",
      "epoch: 61, loss: -0.73810\n",
      "epoch: 62, loss: -0.73015\n",
      "epoch: 63, loss: -0.71554\n",
      "epoch: 64, loss: -0.71418\n",
      "epoch: 65, loss: -0.71853\n",
      "epoch: 66, loss: -0.72034\n",
      "epoch: 67, loss: -0.73644\n",
      "epoch: 68, loss: -0.72962\n",
      "epoch: 69, loss: -0.73583\n",
      "epoch: 70, loss: -0.74342\n",
      "epoch: 71, loss: -0.73355\n",
      "epoch: 72, loss: -0.73997\n",
      "epoch: 73, loss: -0.72568\n",
      "epoch: 74, loss: -0.73284\n",
      "epoch: 75, loss: -0.73642\n",
      "epoch: 76, loss: -0.73997\n",
      "epoch: 77, loss: -0.73602\n",
      "epoch: 78, loss: -0.74186\n",
      "epoch: 79, loss: -0.73652\n",
      "epoch: 80, loss: -0.73284\n",
      "epoch: 81, loss: -0.73463\n",
      "epoch: 82, loss: -0.72916\n",
      "epoch: 83, loss: -0.73394\n",
      "epoch: 84, loss: -0.72559\n",
      "epoch: 85, loss: -0.71411\n",
      "epoch: 86, loss: -0.71321\n",
      "epoch: 87, loss: -0.71304\n",
      "epoch: 88, loss: -0.69681\n",
      "epoch: 89, loss: -0.70996\n",
      "epoch: 90, loss: -0.70781\n",
      "epoch: 91, loss: -0.72014\n",
      "epoch: 92, loss: -0.72159\n",
      "epoch: 93, loss: -0.71313\n",
      "epoch: 94, loss: -0.70274\n",
      "epoch: 95, loss: -0.71021\n",
      "epoch: 96, loss: -0.70773\n",
      "epoch: 97, loss: -0.70132\n",
      "epoch: 98, loss: -0.69746\n",
      "epoch: 99, loss: -0.68590\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [15:35<44:32:53, 161.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "6 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0007.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0007\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48046\n",
      "epoch: 01, loss: -0.64909\n",
      "epoch: 02, loss: -0.67009\n",
      "epoch: 03, loss: -0.68409\n",
      "epoch: 04, loss: -0.69372\n",
      "epoch: 05, loss: -0.70323\n",
      "epoch: 06, loss: -0.70951\n",
      "epoch: 07, loss: -0.71325\n",
      "epoch: 08, loss: -0.70502\n",
      "epoch: 09, loss: -0.70794\n",
      "epoch: 10, loss: -0.71154\n",
      "epoch: 11, loss: -0.71078\n",
      "epoch: 12, loss: -0.71521\n",
      "epoch: 13, loss: -0.71920\n",
      "epoch: 14, loss: -0.72195\n",
      "epoch: 15, loss: -0.72141\n",
      "epoch: 16, loss: -0.72408\n",
      "epoch: 17, loss: -0.71200\n",
      "epoch: 18, loss: -0.72189\n",
      "epoch: 19, loss: -0.73115\n",
      "epoch: 20, loss: -0.73791\n",
      "epoch: 21, loss: -0.74743\n",
      "epoch: 22, loss: -0.76918\n",
      "epoch: 23, loss: -0.77163\n",
      "epoch: 24, loss: -0.76355\n",
      "epoch: 25, loss: -0.76762\n",
      "epoch: 26, loss: -0.79496\n",
      "epoch: 27, loss: -0.81578\n",
      "epoch: 28, loss: -0.81120\n",
      "epoch: 29, loss: -0.79933\n",
      "epoch: 30, loss: -0.79365\n",
      "epoch: 31, loss: -0.81165\n",
      "epoch: 32, loss: -0.80979\n",
      "epoch: 33, loss: -0.82701\n",
      "epoch: 34, loss: -0.83600\n",
      "epoch: 35, loss: -0.83075\n",
      "epoch: 36, loss: -0.80547\n",
      "epoch: 37, loss: -0.78413\n",
      "epoch: 38, loss: -0.80736\n",
      "epoch: 39, loss: -0.79513\n",
      "epoch: 40, loss: -0.79398\n",
      "epoch: 41, loss: -0.80244\n",
      "epoch: 42, loss: -0.80018\n",
      "epoch: 43, loss: -0.78896\n",
      "epoch: 44, loss: -0.79535\n",
      "epoch: 45, loss: -0.81690\n",
      "epoch: 46, loss: -0.83541\n",
      "epoch: 47, loss: -0.84519\n",
      "epoch: 48, loss: -0.84197\n",
      "epoch: 49, loss: -0.83136\n",
      "epoch: 50, loss: -0.81886\n",
      "epoch: 51, loss: -0.82681\n",
      "epoch: 52, loss: -0.82763\n",
      "epoch: 53, loss: -0.83114\n",
      "epoch: 54, loss: -0.83947\n",
      "epoch: 55, loss: -0.85207\n",
      "epoch: 56, loss: -0.84528\n",
      "epoch: 57, loss: -0.83163\n",
      "epoch: 58, loss: -0.80213\n",
      "epoch: 59, loss: -0.79352\n",
      "epoch: 60, loss: -0.81039\n",
      "epoch: 61, loss: -0.80595\n",
      "epoch: 62, loss: -0.78233\n",
      "epoch: 63, loss: -0.77907\n",
      "epoch: 64, loss: -0.78911\n",
      "epoch: 65, loss: -0.79836\n",
      "epoch: 66, loss: -0.80265\n",
      "epoch: 67, loss: -0.79619\n",
      "epoch: 68, loss: -0.79643\n",
      "epoch: 69, loss: -0.82596\n",
      "epoch: 70, loss: -0.84186\n",
      "epoch: 71, loss: -0.83951\n",
      "epoch: 72, loss: -0.83812\n",
      "epoch: 73, loss: -0.84816\n",
      "epoch: 74, loss: -0.86354\n",
      "epoch: 75, loss: -0.86042\n",
      "epoch: 76, loss: -0.86876\n",
      "epoch: 77, loss: -0.87588\n",
      "epoch: 78, loss: -0.87479\n",
      "epoch: 79, loss: -0.86787\n",
      "epoch: 80, loss: -0.87002\n",
      "epoch: 81, loss: -0.88055\n",
      "epoch: 82, loss: -0.88557\n",
      "epoch: 83, loss: -0.88499\n",
      "epoch: 84, loss: -0.88569\n",
      "epoch: 85, loss: -0.87623\n",
      "epoch: 86, loss: -0.86355\n",
      "epoch: 87, loss: -0.87518\n",
      "epoch: 88, loss: -0.87396\n",
      "epoch: 89, loss: -0.88082\n",
      "epoch: 90, loss: -0.88106\n",
      "epoch: 91, loss: -0.87974\n",
      "epoch: 92, loss: -0.87885\n",
      "epoch: 93, loss: -0.88475\n",
      "epoch: 94, loss: -0.90434\n",
      "epoch: 95, loss: -0.90809\n",
      "epoch: 96, loss: -0.90379\n",
      "epoch: 97, loss: -0.90111\n",
      "epoch: 98, loss: -0.89365\n",
      "epoch: 99, loss: -0.88845\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [18:34<46:07:01, 167.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "7 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0008.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0008\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.40559\n",
      "epoch: 01, loss: -0.57269\n",
      "epoch: 02, loss: -0.59957\n",
      "epoch: 03, loss: -0.61882\n",
      "epoch: 04, loss: -0.62550\n",
      "epoch: 05, loss: -0.65947\n",
      "epoch: 06, loss: -0.67040\n",
      "epoch: 07, loss: -0.66913\n",
      "epoch: 08, loss: -0.68108\n",
      "epoch: 09, loss: -0.68893\n",
      "epoch: 10, loss: -0.70963\n",
      "epoch: 11, loss: -0.72626\n",
      "epoch: 12, loss: -0.70468\n",
      "epoch: 13, loss: -0.69475\n",
      "epoch: 14, loss: -0.69116\n",
      "epoch: 15, loss: -0.68423\n",
      "epoch: 16, loss: -0.68119\n",
      "epoch: 17, loss: -0.68181\n",
      "epoch: 18, loss: -0.69641\n",
      "epoch: 19, loss: -0.70079\n",
      "epoch: 20, loss: -0.69089\n",
      "epoch: 21, loss: -0.67518\n",
      "epoch: 22, loss: -0.67616\n",
      "epoch: 23, loss: -0.68325\n",
      "epoch: 24, loss: -0.68312\n",
      "epoch: 25, loss: -0.68120\n",
      "epoch: 26, loss: -0.66586\n",
      "epoch: 27, loss: -0.67113\n",
      "epoch: 28, loss: -0.66799\n",
      "epoch: 29, loss: -0.66144\n",
      "epoch: 30, loss: -0.67179\n",
      "epoch: 31, loss: -0.68250\n",
      "epoch: 32, loss: -0.68864\n",
      "epoch: 33, loss: -0.67907\n",
      "epoch: 34, loss: -0.66462\n",
      "epoch: 35, loss: -0.66905\n",
      "epoch: 36, loss: -0.66178\n",
      "epoch: 37, loss: -0.65402\n",
      "epoch: 38, loss: -0.64611\n",
      "epoch: 39, loss: -0.64503\n",
      "epoch: 40, loss: -0.64942\n",
      "epoch: 41, loss: -0.66088\n",
      "epoch: 42, loss: -0.65716\n",
      "epoch: 43, loss: -0.65042\n",
      "epoch: 44, loss: -0.65978\n",
      "epoch: 45, loss: -0.66202\n",
      "epoch: 46, loss: -0.64902\n",
      "epoch: 47, loss: -0.64918\n",
      "epoch: 48, loss: -0.66158\n",
      "epoch: 49, loss: -0.66111\n",
      "epoch: 50, loss: -0.64008\n",
      "epoch: 51, loss: -0.65118\n",
      "epoch: 52, loss: -0.66483\n",
      "epoch: 53, loss: -0.66003\n",
      "epoch: 54, loss: -0.66111\n",
      "epoch: 55, loss: -0.69203\n",
      "epoch: 56, loss: -0.71788\n",
      "epoch: 57, loss: -0.71595\n",
      "epoch: 58, loss: -0.73767\n",
      "epoch: 59, loss: -0.73575\n",
      "epoch: 60, loss: -0.72829\n",
      "epoch: 61, loss: -0.73931\n",
      "epoch: 62, loss: -0.74511\n",
      "epoch: 63, loss: -0.74105\n",
      "epoch: 64, loss: -0.74534\n",
      "epoch: 65, loss: -0.72795\n",
      "epoch: 66, loss: -0.72635\n",
      "epoch: 67, loss: -0.71451\n",
      "epoch: 68, loss: -0.72248\n",
      "epoch: 69, loss: -0.71295\n",
      "epoch: 70, loss: -0.70611\n",
      "epoch: 71, loss: -0.70282\n",
      "epoch: 72, loss: -0.69390\n",
      "epoch: 73, loss: -0.71200\n",
      "epoch: 74, loss: -0.68772\n",
      "epoch: 75, loss: -0.68768\n",
      "epoch: 76, loss: -0.70906\n",
      "epoch: 77, loss: -0.71221\n",
      "epoch: 78, loss: -0.71399\n",
      "epoch: 79, loss: -0.70618\n",
      "epoch: 80, loss: -0.71298\n",
      "epoch: 81, loss: -0.69589\n",
      "epoch: 82, loss: -0.70035\n",
      "epoch: 83, loss: -0.69730\n",
      "epoch: 84, loss: -0.68357\n",
      "epoch: 85, loss: -0.68591\n",
      "epoch: 86, loss: -0.68695\n",
      "epoch: 87, loss: -0.68031\n",
      "epoch: 88, loss: -0.67979\n",
      "epoch: 89, loss: -0.69095\n",
      "epoch: 90, loss: -0.70357\n",
      "epoch: 91, loss: -0.71554\n",
      "epoch: 92, loss: -0.71517\n",
      "epoch: 93, loss: -0.72402\n",
      "epoch: 94, loss: -0.72226\n",
      "epoch: 95, loss: -0.71708\n",
      "epoch: 96, loss: -0.71071\n",
      "epoch: 97, loss: -0.70431\n",
      "epoch: 98, loss: -0.70434\n",
      "epoch: 99, loss: -0.70605\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [21:33<47:06:13, 170.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "8 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0009.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0009\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.43529\n",
      "epoch: 01, loss: -0.57174\n",
      "epoch: 02, loss: -0.62000\n",
      "epoch: 03, loss: -0.62462\n",
      "epoch: 04, loss: -0.62857\n",
      "epoch: 05, loss: -0.61539\n",
      "epoch: 06, loss: -0.62841\n",
      "epoch: 07, loss: -0.64575\n",
      "epoch: 08, loss: -0.67510\n",
      "epoch: 09, loss: -0.67661\n",
      "epoch: 10, loss: -0.68194\n",
      "epoch: 11, loss: -0.68292\n",
      "epoch: 12, loss: -0.70090\n",
      "epoch: 13, loss: -0.70983\n",
      "epoch: 14, loss: -0.70667\n",
      "epoch: 15, loss: -0.72024\n",
      "epoch: 16, loss: -0.72314\n",
      "epoch: 17, loss: -0.71975\n",
      "epoch: 18, loss: -0.73151\n",
      "epoch: 19, loss: -0.73626\n",
      "epoch: 20, loss: -0.73726\n",
      "epoch: 21, loss: -0.73114\n",
      "epoch: 22, loss: -0.73349\n",
      "epoch: 23, loss: -0.73232\n",
      "epoch: 24, loss: -0.73428\n",
      "epoch: 25, loss: -0.73521\n",
      "epoch: 26, loss: -0.73037\n",
      "epoch: 27, loss: -0.71374\n",
      "epoch: 28, loss: -0.71356\n",
      "epoch: 29, loss: -0.72490\n",
      "epoch: 30, loss: -0.71469\n",
      "epoch: 31, loss: -0.71351\n",
      "epoch: 32, loss: -0.71754\n",
      "epoch: 33, loss: -0.71543\n",
      "epoch: 34, loss: -0.71626\n",
      "epoch: 35, loss: -0.72121\n",
      "epoch: 36, loss: -0.71625\n",
      "epoch: 37, loss: -0.71463\n",
      "epoch: 38, loss: -0.73088\n",
      "epoch: 39, loss: -0.72124\n",
      "epoch: 40, loss: -0.73698\n",
      "epoch: 41, loss: -0.72967\n",
      "epoch: 42, loss: -0.71535\n",
      "epoch: 43, loss: -0.71767\n",
      "epoch: 44, loss: -0.72081\n",
      "epoch: 45, loss: -0.72312\n",
      "epoch: 46, loss: -0.71912\n",
      "epoch: 47, loss: -0.71383\n",
      "epoch: 48, loss: -0.72076\n",
      "epoch: 49, loss: -0.70668\n",
      "epoch: 50, loss: -0.70762\n",
      "epoch: 51, loss: -0.68604\n",
      "epoch: 52, loss: -0.69599\n",
      "epoch: 53, loss: -0.70685\n",
      "epoch: 54, loss: -0.74195\n",
      "epoch: 55, loss: -0.73137\n",
      "epoch: 56, loss: -0.71761\n",
      "epoch: 57, loss: -0.69451\n",
      "epoch: 58, loss: -0.70131\n",
      "epoch: 59, loss: -0.71408\n",
      "epoch: 60, loss: -0.71213\n",
      "epoch: 61, loss: -0.70005\n",
      "epoch: 62, loss: -0.69615\n",
      "epoch: 63, loss: -0.69952\n",
      "epoch: 64, loss: -0.70592\n",
      "epoch: 65, loss: -0.72395\n",
      "epoch: 66, loss: -0.74062\n",
      "epoch: 67, loss: -0.74465\n",
      "epoch: 68, loss: -0.74099\n",
      "epoch: 69, loss: -0.75456\n",
      "epoch: 70, loss: -0.74989\n",
      "epoch: 71, loss: -0.73976\n",
      "epoch: 72, loss: -0.75052\n",
      "epoch: 73, loss: -0.74742\n",
      "epoch: 74, loss: -0.75487\n",
      "epoch: 75, loss: -0.75457\n",
      "epoch: 76, loss: -0.76306\n",
      "epoch: 77, loss: -0.74891\n",
      "epoch: 78, loss: -0.75231\n",
      "epoch: 79, loss: -0.75465\n",
      "epoch: 80, loss: -0.75240\n",
      "epoch: 81, loss: -0.75600\n",
      "epoch: 82, loss: -0.75006\n",
      "epoch: 83, loss: -0.74427\n",
      "epoch: 84, loss: -0.75874\n",
      "epoch: 85, loss: -0.74637\n",
      "epoch: 86, loss: -0.74745\n",
      "epoch: 87, loss: -0.74814\n",
      "epoch: 88, loss: -0.72944\n",
      "epoch: 89, loss: -0.73915\n",
      "epoch: 90, loss: -0.73977\n",
      "epoch: 91, loss: -0.73650\n",
      "epoch: 92, loss: -0.74859\n",
      "epoch: 93, loss: -0.73361\n",
      "epoch: 94, loss: -0.73063\n",
      "epoch: 95, loss: -0.72350\n",
      "epoch: 96, loss: -0.75668\n",
      "epoch: 97, loss: -0.74956\n",
      "epoch: 98, loss: -0.73986\n",
      "epoch: 99, loss: -0.73509\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [24:31<47:40:49, 173.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "9 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0010.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0010\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54632\n",
      "epoch: 01, loss: -0.72254\n",
      "epoch: 02, loss: -0.77068\n",
      "epoch: 03, loss: -0.74806\n",
      "epoch: 04, loss: -0.76000\n",
      "epoch: 05, loss: -0.76478\n",
      "epoch: 06, loss: -0.76153\n",
      "epoch: 07, loss: -0.75986\n",
      "epoch: 08, loss: -0.77208\n",
      "epoch: 09, loss: -0.78299\n",
      "epoch: 10, loss: -0.79405\n",
      "epoch: 11, loss: -0.80390\n",
      "epoch: 12, loss: -0.80543\n",
      "epoch: 13, loss: -0.80348\n",
      "epoch: 14, loss: -0.80455\n",
      "epoch: 15, loss: -0.82207\n",
      "epoch: 16, loss: -0.83377\n",
      "epoch: 17, loss: -0.83362\n",
      "epoch: 18, loss: -0.82953\n",
      "epoch: 19, loss: -0.83238\n",
      "epoch: 20, loss: -0.83534\n",
      "epoch: 21, loss: -0.83677\n",
      "epoch: 22, loss: -0.84124\n",
      "epoch: 23, loss: -0.84076\n",
      "epoch: 24, loss: -0.84083\n",
      "epoch: 25, loss: -0.84219\n",
      "epoch: 26, loss: -0.85533\n",
      "epoch: 27, loss: -0.85016\n",
      "epoch: 28, loss: -0.84661\n",
      "epoch: 29, loss: -0.85297\n",
      "epoch: 30, loss: -0.85529\n",
      "epoch: 31, loss: -0.85284\n",
      "epoch: 32, loss: -0.83982\n",
      "epoch: 33, loss: -0.84312\n",
      "epoch: 34, loss: -0.84504\n",
      "epoch: 35, loss: -0.84698\n",
      "epoch: 36, loss: -0.84956\n",
      "epoch: 37, loss: -0.85103\n",
      "epoch: 38, loss: -0.85320\n",
      "epoch: 39, loss: -0.85250\n",
      "epoch: 40, loss: -0.85564\n",
      "epoch: 41, loss: -0.86288\n",
      "epoch: 42, loss: -0.86526\n",
      "epoch: 43, loss: -0.86811\n",
      "epoch: 44, loss: -0.86561\n",
      "epoch: 45, loss: -0.86270\n",
      "epoch: 46, loss: -0.86098\n",
      "epoch: 47, loss: -0.85504\n",
      "epoch: 48, loss: -0.85238\n",
      "epoch: 49, loss: -0.84935\n",
      "epoch: 50, loss: -0.84514\n",
      "epoch: 51, loss: -0.84105\n",
      "epoch: 52, loss: -0.85356\n",
      "epoch: 53, loss: -0.84913\n",
      "epoch: 54, loss: -0.84869\n",
      "epoch: 55, loss: -0.84075\n",
      "epoch: 56, loss: -0.83848\n",
      "epoch: 57, loss: -0.84412\n",
      "epoch: 58, loss: -0.84758\n",
      "epoch: 59, loss: -0.84202\n",
      "epoch: 60, loss: -0.84887\n",
      "epoch: 61, loss: -0.86749\n",
      "epoch: 62, loss: -0.87016\n",
      "epoch: 63, loss: -0.87632\n",
      "epoch: 64, loss: -0.85320\n",
      "epoch: 65, loss: -0.84492\n",
      "epoch: 66, loss: -0.84249\n",
      "epoch: 67, loss: -0.84750\n",
      "epoch: 68, loss: -0.85760\n",
      "epoch: 69, loss: -0.85876\n",
      "epoch: 70, loss: -0.86053\n",
      "epoch: 71, loss: -0.85972\n",
      "epoch: 72, loss: -0.86492\n",
      "epoch: 73, loss: -0.85548\n",
      "epoch: 74, loss: -0.84928\n",
      "epoch: 75, loss: -0.85100\n",
      "epoch: 76, loss: -0.85121\n",
      "epoch: 77, loss: -0.85450\n",
      "epoch: 78, loss: -0.85438\n",
      "epoch: 79, loss: -0.85800\n",
      "epoch: 80, loss: -0.86496\n",
      "epoch: 81, loss: -0.86087\n",
      "epoch: 82, loss: -0.87054\n",
      "epoch: 83, loss: -0.87141\n",
      "epoch: 84, loss: -0.87218\n",
      "epoch: 85, loss: -0.87068\n",
      "epoch: 86, loss: -0.86987\n",
      "epoch: 87, loss: -0.86048\n",
      "epoch: 88, loss: -0.86774\n",
      "epoch: 89, loss: -0.86034\n",
      "epoch: 90, loss: -0.85058\n",
      "epoch: 91, loss: -0.84227\n",
      "epoch: 92, loss: -0.83879\n",
      "epoch: 93, loss: -0.84634\n",
      "epoch: 94, loss: -0.84703\n",
      "epoch: 95, loss: -0.85575\n",
      "epoch: 96, loss: -0.85236\n",
      "epoch: 97, loss: -0.85168\n",
      "epoch: 98, loss: -0.85914\n",
      "epoch: 99, loss: -0.84358\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [27:39<48:52:24, 177.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "10 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0011.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0011\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53311\n",
      "epoch: 01, loss: -0.69852\n",
      "epoch: 02, loss: -0.72467\n",
      "epoch: 03, loss: -0.73144\n",
      "epoch: 04, loss: -0.73772\n",
      "epoch: 05, loss: -0.72925\n",
      "epoch: 06, loss: -0.73400\n",
      "epoch: 07, loss: -0.73061\n",
      "epoch: 08, loss: -0.74518\n",
      "epoch: 09, loss: -0.74092\n",
      "epoch: 10, loss: -0.75367\n",
      "epoch: 11, loss: -0.75576\n",
      "epoch: 12, loss: -0.77245\n",
      "epoch: 13, loss: -0.78409\n",
      "epoch: 14, loss: -0.78402\n",
      "epoch: 15, loss: -0.78114\n",
      "epoch: 16, loss: -0.78172\n",
      "epoch: 17, loss: -0.78234\n",
      "epoch: 18, loss: -0.78428\n",
      "epoch: 19, loss: -0.78782\n",
      "epoch: 20, loss: -0.78852\n",
      "epoch: 21, loss: -0.79125\n",
      "epoch: 22, loss: -0.80337\n",
      "epoch: 23, loss: -0.80510\n",
      "epoch: 24, loss: -0.80034\n",
      "epoch: 25, loss: -0.79986\n",
      "epoch: 26, loss: -0.79000\n",
      "epoch: 27, loss: -0.78339\n",
      "epoch: 28, loss: -0.79211\n",
      "epoch: 29, loss: -0.79494\n",
      "epoch: 30, loss: -0.78891\n",
      "epoch: 31, loss: -0.78441\n",
      "epoch: 32, loss: -0.79076\n",
      "epoch: 33, loss: -0.78329\n",
      "epoch: 34, loss: -0.78336\n",
      "epoch: 35, loss: -0.77768\n",
      "epoch: 36, loss: -0.77518\n",
      "epoch: 37, loss: -0.77700\n",
      "epoch: 38, loss: -0.76801\n",
      "epoch: 39, loss: -0.76951\n",
      "epoch: 40, loss: -0.76700\n",
      "epoch: 41, loss: -0.77457\n",
      "epoch: 42, loss: -0.78448\n",
      "epoch: 43, loss: -0.76732\n",
      "epoch: 44, loss: -0.74886\n",
      "epoch: 45, loss: -0.74136\n",
      "epoch: 46, loss: -0.75533\n",
      "epoch: 47, loss: -0.75063\n",
      "epoch: 48, loss: -0.76013\n",
      "epoch: 49, loss: -0.76367\n",
      "epoch: 50, loss: -0.75491\n",
      "epoch: 51, loss: -0.76501\n",
      "epoch: 52, loss: -0.78218\n",
      "epoch: 53, loss: -0.76992\n",
      "epoch: 54, loss: -0.76008\n",
      "epoch: 55, loss: -0.77389\n",
      "epoch: 56, loss: -0.77213\n",
      "epoch: 57, loss: -0.78676\n",
      "epoch: 58, loss: -0.78714\n",
      "epoch: 59, loss: -0.76177\n",
      "epoch: 60, loss: -0.76992\n",
      "epoch: 61, loss: -0.77075\n",
      "epoch: 62, loss: -0.76751\n"
     ]
    }
   ],
   "source": [
    "utils.make_output_dir(output_dir)\n",
    "feat_list=[]\n",
    "inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "for inp in tqdm(inputs):\n",
    "    index, features_file = inp\n",
    "    print(index, features_file)\n",
    "     # Load\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    print(data_dict.keys())   #['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape']\n",
    "    # print(\"shape=\", data_dict['shape'], \"k shape\", data_dict['k'].shape, \"patch_size=\", data_dict['patch_size'])\n",
    "    image_id = data_dict['file'][:-4]\n",
    "    print(image_id)\n",
    "    # Load\n",
    "    output_file = str(Path(output_dir) / f'{image_id}.pth')\n",
    "    if Path(output_file).is_file():\n",
    "        print(f'Skipping existing file {str(output_file)}')\n",
    "        # break\n",
    "        # return  # skip because already generated\n",
    "\n",
    "    # Load affinity matrix\n",
    "    feats = data_dict[which_features].squeeze().cuda()\n",
    "    # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "    if normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "    # print(\"After normalization, Features Shape\",feats.shape)\n",
    "    # print(\"which_matrix=\", which_matrix)\n",
    "    # Eigenvectors of affinity matrix\n",
    "    if which_matrix == 'affinity_torch':\n",
    "        W = feats @ feats.T\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "            # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = torch.eig(W, eigenvectors=True)\n",
    "        eigenvalues = eigenvalues.cpu()\n",
    "        eigenvectors = eigenvectors.cpu()\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity_svd':\n",
    "        USV = torch.linalg.svd(feats, full_matrices=False)\n",
    "        eigenvectors = USV[0][:, :K].T.to('cpu', non_blocking=True)\n",
    "        eigenvalues = USV[1][:K].to('cpu', non_blocking=True)\n",
    "        print(\"which matrix=\",which_matrix,\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity':\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "        W = (feats @ feats.T)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "        W = W.cpu().numpy()\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = eigsh(W, which='LM', k=K)\n",
    "        eigenvectors = torch.flip(torch.from_numpy(eigenvectors), dims=(-1,)).T\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of matting laplacian matrix\n",
    "    elif which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "\n",
    "        # Get sizes\n",
    "        B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "        if image_downsample_factor is None:\n",
    "            image_downsample_factor = P\n",
    "        H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "\n",
    "        # Upscale features to match the resolution\n",
    "        if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "            feats = F.interpolate(\n",
    "                feats.T.reshape(1, -1, H_patch, W_patch),\n",
    "                size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "            ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "\n",
    "        ### Feature affinities\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "\n",
    "        W_feat_ds = (feats @ feats.T)\n",
    "        # feat_list.append(feats)\n",
    "        feat_dataset = Feature_Dataset(feats)\n",
    "        if feats.shape[0]%2==0:\n",
    "            features_dataloader = DataLoader(feat_dataset, batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            features_dataloader = DataLoader(feat_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        model_simsiam = SimSiam()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_simsiam.to(device)\n",
    "        criterion = NegativeCosineSimilarity()\n",
    "        optimizer = torch.optim.SGD(model_simsiam.parameters(), lr=0.06)\n",
    "        print(\"Starting Training\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x0 in features_dataloader:\n",
    "            # for (x0), _, _ in features_dataloader:\n",
    "            #     print(\"Before Unsqueezing, x0 shape=\", x0.shape)\n",
    "                x0 = x0.unsqueeze(0).to(device)\n",
    "                x0 = x0.unsqueeze(1).to(device)\n",
    "                # print(\"After Unsqueezing x0 shape=\", x0.shape)\n",
    "                x1=torchvision.transforms.RandomAffine(0)(x0)\n",
    "                # print(\"After Unsqueezing x1 shape=\", x1.shape)\n",
    "                # x0 = x0.squeeze(0).to(device)\n",
    "                # print(\"batch_size=\",batch_size)\n",
    "                # x0_new = torch.tensor(x0).view(batch_size, 1, 1, 384)\n",
    "                # x0_new = torch.tensor(x0).view(batch_size, 1, 384)\n",
    "                x0_new = x0.view(batch_size, 1, 1, 384)\n",
    "                # print(\"After viewing x0 shape=\", x0_new.shape)\n",
    "                # print(\"x0.shape=\", x0.shape)\n",
    "                # x1 = x1.squeeze(0).to(device)\n",
    "                # x1 = torch.tensor(x1).view(batch_size, 1, 1,384)\n",
    "                x1_new = x1.view(batch_size, 1, 1, 384)\n",
    "                # print(\"After viewing x1 shape=\", x1_new.shape)\n",
    "                z0, p0 = model_simsiam(x0_new)\n",
    "                z1, p1 = model_simsiam(x1_new)\n",
    "                loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "                total_loss += loss.detach()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_loss = total_loss / len(features_dataloader)\n",
    "            print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "        feats=feats.unsqueeze(1).to(device)\n",
    "        feats=feats.unsqueeze(2).to(device)\n",
    "        print(\"After Unsqueezing, feature size=\", feats.shape)\n",
    "        projected_feature=model_simsiam(feats)\n",
    "        W_feat_siam=torch.matmul(projected_feature[0], projected_feature[0].t())\n",
    "        W_feat=W_feat_ds + W_feat_siam\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        if threshold_at_zero:\n",
    "            W_feat = (W_feat * (W_feat > 0))\n",
    "        W_feat = W_feat / W_feat.max()  # NOTE: If features are normalized, this naturally does nothing\n",
    "        # W_feat = W_feat.cpu().numpy()\n",
    "        W_feat = W_feat.detach().cpu().numpy()\n",
    "        # print(\"W_feat shape=\",W_feat.shape)\n",
    "\n",
    "        ### Color affinities\n",
    "        # If we are fusing with color affinites, then load the image and compute\n",
    "        if image_color_lambda > 0:\n",
    "\n",
    "            # Load image\n",
    "            image_file = str(Path(images_root) / f'{image_id}.jpg')\n",
    "            image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "            image_lr = np.array(image_lr) / 255.\n",
    "\n",
    "            # Color affinities (of type scipy.sparse.csr_matrix)\n",
    "            if which_color_matrix == 'knn':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "            elif which_color_matrix == 'rw':\n",
    "                W_lr = utils.rw_affinity(image_lr / 255)\n",
    "\n",
    "            # Convert to dense numpy array\n",
    "            W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "            # print(\"W_color shape\", W_color.shape)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # No color affinity\n",
    "            W_color = 0\n",
    "\n",
    "        # Combine\n",
    "        W_comb = W_feat + W_color * image_color_lambda  # combination\n",
    "        D_comb = np.array(utils.get_diagonal(W_comb).todense())  # is dense or sparse faster? not sure, should check\n",
    "        # print(\"W_comb shape= \", W_comb.shape, \"D_comb shape\",  D_comb.shape)\n",
    "        if lapnorm:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM', M=D_comb)\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM', M=D_comb)\n",
    "        else:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM')\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM')\n",
    "        eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "    print(\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "    # Sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "\n",
    "    # Save dict\n",
    "    output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "    torch.save(output_dict, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}