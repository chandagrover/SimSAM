{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "from pathlib import Path\n",
    "# from typing import Optional, Tuple\n",
    "# import cv2\n",
    "# import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "# from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "import extract_utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from torch import nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# images_list=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/lists/images.txt\"\n",
    "# images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/images\"\n",
    "# model_name=\"dino_vits16\"\n",
    "# batch_size=1\n",
    "# output_dir=\"//home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16\"\n",
    "# which_block=-1\n",
    "# # Output\n",
    "# utils.make_output_dir(output_dir)\n",
    "# # Models\n",
    "# model_name = model_name.lower()\n",
    "# model, val_transform, patch_size, num_heads = utils.get_model(model_name)    #patch size= 16 number of heads= 6\n",
    "# # print(\"patch size=\", patch_size, \"number of heads=\", num_heads)\n",
    "# # Add hook\n",
    "# if 'dino' in model_name or 'mocov3' in model_name:\n",
    "#     feat_out = {}\n",
    "#     def hook_fn_forward_qkv(module, input, output):\n",
    "#         # print(\"feat_out.keys()\", feat_out.keys())\n",
    "#         feat_out[\"qkv\"] = output\n",
    "#     model._modules[\"blocks\"][which_block]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "# else:\n",
    "#     raise ValueError(model_name)\n",
    "# # Dataset\n",
    "# filenames = Path(images_list).read_text().splitlines()\n",
    "# dataset = utils.ImagesDataset(filenames=filenames, images_root=images_root, transform=val_transform)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8)\n",
    "# # print(f'Dataset size: {len(dataset)=}')\n",
    "# # print(f'Dataloader size: {len(dataloader)=}')\n",
    "# # Prepare\n",
    "# # accelerator = Accelerator(fp16=True, cpu=False)\n",
    "# accelerator = Accelerator(cpu=False)\n",
    "# # model, dataloader = accelerator.prepare(model, dataloader)\n",
    "# model = model.to(accelerator.device)\n",
    "# model.num_features\n",
    "# # print(\"model.num_features=\", model.num_features)\n",
    "# # model.get_submodule\n",
    "# # Process\n",
    "# pbar = list(tqdm(dataloader, desc='Processing'))\n",
    "# # Process\n",
    "# # pbar = list(tqdm(dataloader, desc='Processing'))\n",
    "# # print(\"feat_out.keys()=\", feat_out.keys())\n",
    "# for i, (images, files, indices) in enumerate(pbar):\n",
    "#     output_dict = {}\n",
    "#     # print(\"images.shape=\", images.shape, \"files =\", files, \"indices\", indices)\n",
    "#\n",
    "#     # Check if file already exists\n",
    "#     id = Path(files[0]).stem\n",
    "#     # print(\"id=\", id)\n",
    "#     output_file = Path(output_dir) / f'{id}.pth'\n",
    "#     # if output_file.is_file():\n",
    "#     #     pbar.write(f'Skipping existing file {str(output_file)}')\n",
    "#     #     continue\n",
    "#\n",
    "#     # Reshape image\n",
    "#     P = patch_size\n",
    "#     B, C, H, W = images.shape\n",
    "#     H_patch, W_patch = H // P, W // P\n",
    "#     H_pad, W_pad = H_patch * P, W_patch * P\n",
    "#     T = H_patch * W_patch + 1  # number of tokens, add 1 for [CLS]\n",
    "#     # images = F.interpolate(images, size=(H_pad, W_pad), mode='bilinear')  # resize image\n",
    "#     images = images[:, :, :H_pad, :W_pad]\n",
    "#     images = images.to(accelerator.device)\n",
    "#     # print(\"images.shape after padding=\", images.shape)\n",
    "#\n",
    "#     # Forward and collect features into output dict\n",
    "#     if 'dino' in model_name or 'mocov3' in model_name:\n",
    "#         # accelerator.unwrap_model(model).get_intermediate_layers(images)[0].squeeze(0)\n",
    "#         model.get_intermediate_layers(images)[0].squeeze(0)\n",
    "#         # print(model.get_intermediate_layers(images)[0])\n",
    "#         # output_dict['out'] = out\n",
    "#         # print(\"feat_out.keys()=\", feat_out.keys())\n",
    "#         output_qkv = feat_out[\"qkv\"].reshape(B, T, 3, num_heads, -1 // num_heads).permute(2, 0, 3, 1, 4)\n",
    "#         # print(\"type(output_qkv)\", type(output_qkv))\n",
    "#         # print(\"output_qkv.shape\", output_qkv.shape)    #3, 1, 6, 931, 64]\n",
    "#         # print(\"output_qkv[0].shape\", output_qkv[0].shape)\n",
    "#         # print(\"output_qkv[1].shape\", output_qkv[1].shape)\n",
    "#         # print(\"output_qkv[2].shape\", output_qkv[2].shape)\n",
    "#         # output_dict['q'] = output_qkv[0].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#         output_dict['k'] = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#         # print(\"output_dict[k].shape=\", output_dict['k'].shape)\n",
    "#         # output_dict['v'] = output_qkv[2].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "#     else:\n",
    "#         raise ValueError(model_name)\n",
    "#\n",
    "#     # print(\"output_dict.items=\", output_dict.items())\n",
    "#     # Metadata\n",
    "#     output_dict['indices'] = indices[0]\n",
    "#     output_dict['file'] = files[0]\n",
    "#     output_dict['id'] = id\n",
    "#     output_dict['model_name'] = model_name\n",
    "#     output_dict['patch_size'] = patch_size\n",
    "#     output_dict['shape'] = (B, C, H, W)\n",
    "#     output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "#     # for k, v in output_dict.items():\n",
    "#     #     print(\"k=\", k)\n",
    "#     #     if torch.is_tensor(v):\n",
    "#     #         print(\"v.shape\", v.shape)\n",
    "#     #     else:\n",
    "#     #         print(\"v=\", v)\n",
    "#     # print(\"output_dict.keys()\", output_dict.keys())\n",
    "#     # print(\"output_dict['k'].shape=\", output_dict['k'].shape,\"output_dict['indices'] =\", indices[0],\"output_dict['file'] =\", files[0],\"output_dict['id']=\" , id, \"output_dict['model_name'] =\", model_name,\" output_dict['shape'] =(\", B, C, H, W,\")output_dict['patch_size'] =\",  patch_size )\n",
    "#     # Save\n",
    "#     accelerator.save(output_dict, str(output_file))\n",
    "#     accelerator.wait_for_everyone()\n",
    "# print(f'Saved features to {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "# print(output_dict.keys())\n",
    "# for k, v in output_dict.items():\n",
    "#     print(\"k=\", k)\n",
    "#     print(\"v.shape=\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/images\"\n",
    "features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16\"\n",
    "output_dir=\"//home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/eigs_dot1residualsimsiam_ds_10_jn\"\n",
    "which_matrix= 'laplacian'\n",
    "which_color_matrix= 'knn'\n",
    "which_features= 'k'\n",
    "normalize=True\n",
    "threshold_at_zero=True\n",
    "lapnorm= True\n",
    "K= 5\n",
    "image_downsample_factor = None\n",
    "image_color_lambda = 0.0\n",
    "multiprocessing = 0\n",
    "batch_size=2\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet Residual Block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(128, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        # print(\"Before squeezing, out shape=\", out.shape)\n",
    "        out =  out.squeeze().to('cuda')\n",
    "        # print(\"After squeezing, out shape=\", out.shape)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# class ResidualBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n",
    "#         super(ResidualBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         # shortcut connection\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=bias),\n",
    "#                 nn.BatchNorm1d(out_channels)\n",
    "#             )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         residual = x\n",
    "#\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#\n",
    "#         out += self.shortcut(residual)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# class ResNetBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "#         super(ResNetBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "#         self.downsample = downsample\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "#\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Incorporating SimSiam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Feature_Dataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# print(feats.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.projection_head = SimSiamProjectionHead(feats.shape[1], 128,feats.shape[1])\n",
    "        self.projection_head = BasicBlock()\n",
    "        # self.projection_head = ResidualBlock(feats.shape[1], feats.shape[1])\n",
    "        self.prediction_head = SimSiamPredictionHead(feats.shape[1], 128, feats.shape[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.projection_head(x)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# utils.make_output_dir(output_dir)\n",
    "# feat_list=[]\n",
    "# inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "# batch_size=2\n",
    "# token_feature_size=384\n",
    "# for inp in tqdm(inputs[:2]):\n",
    "#     index, features_file = inp\n",
    "#     print(index, features_file)\n",
    "#      # Load\n",
    "#     data_dict = torch.load(features_file, map_location='cpu')\n",
    "#     # Load affinity matrix\n",
    "#     # feats_unsqueeze=data_dict[which_features].cuda()\n",
    "#     feats = data_dict[which_features].squeeze().cuda()\n",
    "#     print(feats.shape)\n",
    "#     # print(feats_unsqueeze.shape)\n",
    "#     feat_dataset = Feature_Dataset(feats)\n",
    "#     features_dataloader = DataLoader(feat_dataset, batch_size=2, shuffle=True)\n",
    "#     i=0\n",
    "#     for x0 in features_dataloader:\n",
    "#         if i==0:\n",
    "#         # for (x0), _, _ in features_dataloader:\n",
    "#             print(\"before unsqueezing x0.shape=\", x0.shape)\n",
    "#             x0 = x0.unsqueeze(0).to(device)\n",
    "#             x1=torchvision.transforms.RandomAffine(0)(x0)\n",
    "#             x0 = x0.squeeze(0).to(device)\n",
    "#             x0 = torch.tensor(x0).view(batch_size, 1, 1, token_feature_size)\n",
    "#             print(\"After squeezing and viewing x0 shape=\", x0.shape)\n",
    "#             # print(\"x0.shape=\", x0.shape)\n",
    "#             x1 = x1.squeeze(0).to(device)\n",
    "#             x1 = torch.tensor(x1).view(batch_size, 1, 1,token_feature_size)\n",
    "#             print(\"After squeezing and viewing x0 shape=\", x1.shape)\n",
    "#             print(\"x1 shape=\", x1.shape)\n",
    "#         else:\n",
    "#             break\n",
    "#         i+=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0001.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0001\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.47559\n",
      "epoch: 01, loss: -0.64990\n",
      "epoch: 02, loss: -0.68179\n",
      "epoch: 03, loss: -0.70850\n",
      "epoch: 04, loss: -0.72402\n",
      "epoch: 05, loss: -0.72735\n",
      "epoch: 06, loss: -0.73119\n",
      "epoch: 07, loss: -0.73327\n",
      "epoch: 08, loss: -0.74460\n",
      "epoch: 09, loss: -0.76263\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:21<5:49:55, 21.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "1 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0002.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0002\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.47327\n",
      "epoch: 01, loss: -0.62885\n",
      "epoch: 02, loss: -0.65586\n",
      "epoch: 03, loss: -0.66614\n",
      "epoch: 04, loss: -0.68491\n",
      "epoch: 05, loss: -0.70357\n",
      "epoch: 06, loss: -0.69230\n",
      "epoch: 07, loss: -0.68387\n",
      "epoch: 08, loss: -0.69898\n",
      "epoch: 09, loss: -0.68993\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:39<5:26:33, 19.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "2 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0003.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0003\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50733\n",
      "epoch: 01, loss: -0.66382\n",
      "epoch: 02, loss: -0.68148\n",
      "epoch: 03, loss: -0.69327\n",
      "epoch: 04, loss: -0.69657\n",
      "epoch: 05, loss: -0.70834\n",
      "epoch: 06, loss: -0.71802\n",
      "epoch: 07, loss: -0.73357\n",
      "epoch: 08, loss: -0.73374\n",
      "epoch: 09, loss: -0.73411\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:56<5:06:33, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "3 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0004.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0004\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53601\n",
      "epoch: 01, loss: -0.69542\n",
      "epoch: 02, loss: -0.72697\n",
      "epoch: 03, loss: -0.73491\n",
      "epoch: 04, loss: -0.73247\n",
      "epoch: 05, loss: -0.74163\n",
      "epoch: 06, loss: -0.74971\n",
      "epoch: 07, loss: -0.76014\n",
      "epoch: 08, loss: -0.76289\n",
      "epoch: 09, loss: -0.76647\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [01:14<5:02:55, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "4 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0005.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0005\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48935\n",
      "epoch: 01, loss: -0.62943\n",
      "epoch: 02, loss: -0.65905\n",
      "epoch: 03, loss: -0.66292\n",
      "epoch: 04, loss: -0.67389\n",
      "epoch: 05, loss: -0.68233\n",
      "epoch: 06, loss: -0.69151\n",
      "epoch: 07, loss: -0.69591\n",
      "epoch: 08, loss: -0.69151\n",
      "epoch: 09, loss: -0.70216\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [01:33<5:06:00, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "5 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0006.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0006\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50634\n",
      "epoch: 01, loss: -0.65195\n",
      "epoch: 02, loss: -0.66642\n",
      "epoch: 03, loss: -0.68174\n",
      "epoch: 04, loss: -0.68453\n",
      "epoch: 05, loss: -0.69048\n",
      "epoch: 06, loss: -0.69004\n",
      "epoch: 07, loss: -0.67853\n",
      "epoch: 08, loss: -0.68186\n",
      "epoch: 09, loss: -0.69407\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [01:56<5:30:07, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "6 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0007.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0007\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51947\n",
      "epoch: 01, loss: -0.67091\n",
      "epoch: 02, loss: -0.68763\n",
      "epoch: 03, loss: -0.68785\n",
      "epoch: 04, loss: -0.70070\n",
      "epoch: 05, loss: -0.69388\n",
      "epoch: 06, loss: -0.69418\n",
      "epoch: 07, loss: -0.70406\n",
      "epoch: 08, loss: -0.71478\n",
      "epoch: 09, loss: -0.73452\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [02:18<5:41:36, 20.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "7 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0008.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0008\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51320\n",
      "epoch: 01, loss: -0.65179\n",
      "epoch: 02, loss: -0.68106\n",
      "epoch: 03, loss: -0.68028\n",
      "epoch: 04, loss: -0.68528\n",
      "epoch: 05, loss: -0.66059\n",
      "epoch: 06, loss: -0.66309\n",
      "epoch: 07, loss: -0.65948\n",
      "epoch: 08, loss: -0.66741\n",
      "epoch: 09, loss: -0.69091\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [02:39<5:43:14, 20.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "8 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0009.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0009\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.49326\n",
      "epoch: 01, loss: -0.64836\n",
      "epoch: 02, loss: -0.70149\n",
      "epoch: 03, loss: -0.72735\n",
      "epoch: 04, loss: -0.71370\n",
      "epoch: 05, loss: -0.71325\n",
      "epoch: 06, loss: -0.73165\n",
      "epoch: 07, loss: -0.72637\n",
      "epoch: 08, loss: -0.72352\n",
      "epoch: 09, loss: -0.73918\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [02:59<5:37:51, 20.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "9 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0010.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0010\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53478\n",
      "epoch: 01, loss: -0.69762\n",
      "epoch: 02, loss: -0.73410\n",
      "epoch: 03, loss: -0.76242\n",
      "epoch: 04, loss: -0.77078\n",
      "epoch: 05, loss: -0.79249\n",
      "epoch: 06, loss: -0.79354\n",
      "epoch: 07, loss: -0.80238\n",
      "epoch: 08, loss: -0.80660\n",
      "epoch: 09, loss: -0.81373\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [03:16<5:19:56, 19.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "10 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0011.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0011\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53309\n",
      "epoch: 01, loss: -0.69051\n",
      "epoch: 02, loss: -0.70900\n",
      "epoch: 03, loss: -0.70927\n",
      "epoch: 04, loss: -0.73120\n",
      "epoch: 05, loss: -0.74152\n",
      "epoch: 06, loss: -0.74148\n",
      "epoch: 07, loss: -0.75265\n",
      "epoch: 08, loss: -0.76346\n",
      "epoch: 09, loss: -0.76667\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [03:36<5:25:15, 19.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "11 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0012.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0012\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54960\n",
      "epoch: 01, loss: -0.74311\n",
      "epoch: 02, loss: -0.77301\n",
      "epoch: 03, loss: -0.78136\n",
      "epoch: 04, loss: -0.79719\n",
      "epoch: 05, loss: -0.80276\n",
      "epoch: 06, loss: -0.80823\n",
      "epoch: 07, loss: -0.81809\n",
      "epoch: 08, loss: -0.82772\n",
      "epoch: 09, loss: -0.83114\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [03:58<5:36:35, 20.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "12 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0013.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0013\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48603\n",
      "epoch: 01, loss: -0.65786\n",
      "epoch: 02, loss: -0.67801\n",
      "epoch: 03, loss: -0.69717\n",
      "epoch: 04, loss: -0.70275\n",
      "epoch: 05, loss: -0.70687\n",
      "epoch: 06, loss: -0.71277\n",
      "epoch: 07, loss: -0.69905\n",
      "epoch: 08, loss: -0.70135\n",
      "epoch: 09, loss: -0.69808\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [04:19<5:36:49, 20.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "13 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0014.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0014\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48158\n",
      "epoch: 01, loss: -0.62454\n",
      "epoch: 02, loss: -0.65661\n",
      "epoch: 03, loss: -0.66984\n",
      "epoch: 04, loss: -0.68448\n",
      "epoch: 05, loss: -0.68605\n",
      "epoch: 06, loss: -0.70764\n",
      "epoch: 07, loss: -0.70670\n",
      "epoch: 08, loss: -0.69898\n",
      "epoch: 09, loss: -0.68738\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/1000 [04:39<5:33:14, 20.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "14 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0015.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0015\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.49047\n",
      "epoch: 01, loss: -0.66344\n",
      "epoch: 02, loss: -0.68431\n",
      "epoch: 03, loss: -0.70443\n",
      "epoch: 04, loss: -0.69994\n",
      "epoch: 05, loss: -0.69018\n",
      "epoch: 06, loss: -0.69872\n",
      "epoch: 07, loss: -0.70381\n",
      "epoch: 08, loss: -0.71633\n",
      "epoch: 09, loss: -0.72902\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [04:58<5:27:56, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "15 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0016.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0016\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.44409\n",
      "epoch: 01, loss: -0.57055\n",
      "epoch: 02, loss: -0.62491\n",
      "epoch: 03, loss: -0.62506\n",
      "epoch: 04, loss: -0.65507\n",
      "epoch: 05, loss: -0.65542\n",
      "epoch: 06, loss: -0.66879\n",
      "epoch: 07, loss: -0.67461\n",
      "epoch: 08, loss: -0.67880\n",
      "epoch: 09, loss: -0.67826\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [05:17<5:22:53, 19.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "16 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0017.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0017\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.58085\n",
      "epoch: 01, loss: -0.72643\n",
      "epoch: 02, loss: -0.74444\n",
      "epoch: 03, loss: -0.75400\n",
      "epoch: 04, loss: -0.76677\n",
      "epoch: 05, loss: -0.76306\n",
      "epoch: 06, loss: -0.76084\n",
      "epoch: 07, loss: -0.75821\n",
      "epoch: 08, loss: -0.76296\n",
      "epoch: 09, loss: -0.76189\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1000 [05:37<5:26:49, 19.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "17 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0018.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0018\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.59754\n",
      "epoch: 01, loss: -0.76774\n",
      "epoch: 02, loss: -0.79265\n",
      "epoch: 03, loss: -0.79799\n",
      "epoch: 04, loss: -0.80695\n",
      "epoch: 05, loss: -0.81821\n",
      "epoch: 06, loss: -0.82393\n",
      "epoch: 07, loss: -0.82387\n",
      "epoch: 08, loss: -0.83621\n",
      "epoch: 09, loss: -0.83762\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [05:58<5:29:33, 20.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "18 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0019.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0019\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50687\n",
      "epoch: 01, loss: -0.66348\n",
      "epoch: 02, loss: -0.67687\n",
      "epoch: 03, loss: -0.68937\n",
      "epoch: 04, loss: -0.69273\n",
      "epoch: 05, loss: -0.69978\n",
      "epoch: 06, loss: -0.69544\n",
      "epoch: 07, loss: -0.69696\n",
      "epoch: 08, loss: -0.70506\n",
      "epoch: 09, loss: -0.70520\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [06:16<5:19:08, 19.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "19 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0020.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0020\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.47068\n",
      "epoch: 01, loss: -0.63072\n",
      "epoch: 02, loss: -0.65244\n",
      "epoch: 03, loss: -0.64706\n",
      "epoch: 04, loss: -0.65531\n",
      "epoch: 05, loss: -0.66680\n",
      "epoch: 06, loss: -0.67079\n",
      "epoch: 07, loss: -0.68410\n",
      "epoch: 08, loss: -0.69248\n",
      "epoch: 09, loss: -0.70912\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1000 [06:37<5:26:03, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "20 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0021.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0021\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.55170\n",
      "epoch: 01, loss: -0.72016\n",
      "epoch: 02, loss: -0.73178\n",
      "epoch: 03, loss: -0.74354\n",
      "epoch: 04, loss: -0.76303\n",
      "epoch: 05, loss: -0.76986\n",
      "epoch: 06, loss: -0.76982\n",
      "epoch: 07, loss: -0.76685\n",
      "epoch: 08, loss: -0.77923\n",
      "epoch: 09, loss: -0.78542\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [06:57<5:24:04, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "21 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0022.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0022\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.55534\n",
      "epoch: 01, loss: -0.72523\n",
      "epoch: 02, loss: -0.72856\n",
      "epoch: 03, loss: -0.73642\n",
      "epoch: 04, loss: -0.73449\n",
      "epoch: 05, loss: -0.74257\n",
      "epoch: 06, loss: -0.75115\n",
      "epoch: 07, loss: -0.74840\n",
      "epoch: 08, loss: -0.75090\n",
      "epoch: 09, loss: -0.76837\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [07:18<5:29:14, 20.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "22 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0023.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0023\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53500\n",
      "epoch: 01, loss: -0.68934\n",
      "epoch: 02, loss: -0.70946\n",
      "epoch: 03, loss: -0.71043\n",
      "epoch: 04, loss: -0.71657\n",
      "epoch: 05, loss: -0.73037\n",
      "epoch: 06, loss: -0.74184\n",
      "epoch: 07, loss: -0.75002\n",
      "epoch: 08, loss: -0.75487\n",
      "epoch: 09, loss: -0.75945\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1000 [07:37<5:22:54, 19.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "23 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0024.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0024\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50772\n",
      "epoch: 01, loss: -0.68410\n",
      "epoch: 02, loss: -0.68447\n",
      "epoch: 03, loss: -0.68634\n",
      "epoch: 04, loss: -0.68731\n",
      "epoch: 05, loss: -0.68212\n",
      "epoch: 06, loss: -0.68029\n",
      "epoch: 07, loss: -0.71275\n",
      "epoch: 08, loss: -0.72089\n",
      "epoch: 09, loss: -0.73470\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [07:57<5:25:35, 20.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "24 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0025.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0025\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.47512\n",
      "epoch: 01, loss: -0.63195\n",
      "epoch: 02, loss: -0.65948\n",
      "epoch: 03, loss: -0.67775\n",
      "epoch: 04, loss: -0.68991\n",
      "epoch: 05, loss: -0.69971\n",
      "epoch: 06, loss: -0.70648\n",
      "epoch: 07, loss: -0.71146\n",
      "epoch: 08, loss: -0.70660\n",
      "epoch: 09, loss: -0.70775\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [08:17<5:24:45, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "25 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0026.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0026\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54586\n",
      "epoch: 01, loss: -0.69816\n",
      "epoch: 02, loss: -0.71754\n",
      "epoch: 03, loss: -0.72766\n",
      "epoch: 04, loss: -0.72645\n",
      "epoch: 05, loss: -0.73119\n",
      "epoch: 06, loss: -0.73029\n",
      "epoch: 07, loss: -0.73896\n",
      "epoch: 08, loss: -0.71747\n",
      "epoch: 09, loss: -0.72580\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26/1000 [08:38<5:27:14, 20.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "26 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0027.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0027\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.49830\n",
      "epoch: 01, loss: -0.66188\n",
      "epoch: 02, loss: -0.68709\n",
      "epoch: 03, loss: -0.69784\n",
      "epoch: 04, loss: -0.69121\n",
      "epoch: 05, loss: -0.70626\n",
      "epoch: 06, loss: -0.68795\n",
      "epoch: 07, loss: -0.70013\n",
      "epoch: 08, loss: -0.69517\n",
      "epoch: 09, loss: -0.69403\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/1000 [08:57<5:23:23, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "27 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0028.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0028\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53524\n",
      "epoch: 01, loss: -0.68016\n",
      "epoch: 02, loss: -0.70493\n",
      "epoch: 03, loss: -0.71455\n",
      "epoch: 04, loss: -0.72549\n",
      "epoch: 05, loss: -0.74200\n",
      "epoch: 06, loss: -0.74581\n",
      "epoch: 07, loss: -0.75634\n",
      "epoch: 08, loss: -0.75930\n",
      "epoch: 09, loss: -0.76508\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/1000 [09:17<5:24:57, 20.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "28 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0029.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0029\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48255\n",
      "epoch: 01, loss: -0.64564\n",
      "epoch: 02, loss: -0.67588\n",
      "epoch: 03, loss: -0.68689\n",
      "epoch: 04, loss: -0.69933\n",
      "epoch: 05, loss: -0.71133\n",
      "epoch: 06, loss: -0.71338\n",
      "epoch: 07, loss: -0.71285\n",
      "epoch: 08, loss: -0.72285\n",
      "epoch: 09, loss: -0.73562\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/1000 [09:37<5:23:53, 20.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "29 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0030.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0030\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.49988\n",
      "epoch: 01, loss: -0.66164\n",
      "epoch: 02, loss: -0.68172\n",
      "epoch: 03, loss: -0.67836\n",
      "epoch: 04, loss: -0.71269\n",
      "epoch: 05, loss: -0.74451\n",
      "epoch: 06, loss: -0.75044\n",
      "epoch: 07, loss: -0.75746\n",
      "epoch: 08, loss: -0.76198\n",
      "epoch: 09, loss: -0.76040\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1000 [09:58<5:25:13, 20.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "30 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0031.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0031\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.45821\n",
      "epoch: 01, loss: -0.62038\n",
      "epoch: 02, loss: -0.64857\n",
      "epoch: 03, loss: -0.67364\n",
      "epoch: 04, loss: -0.69047\n",
      "epoch: 05, loss: -0.69740\n",
      "epoch: 06, loss: -0.69686\n",
      "epoch: 07, loss: -0.69365\n",
      "epoch: 08, loss: -0.69771\n",
      "epoch: 09, loss: -0.69521\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [10:18<5:24:40, 20.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "31 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0032.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0032\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50847\n",
      "epoch: 01, loss: -0.64294\n",
      "epoch: 02, loss: -0.68085\n",
      "epoch: 03, loss: -0.69539\n",
      "epoch: 04, loss: -0.69575\n",
      "epoch: 05, loss: -0.68965\n",
      "epoch: 06, loss: -0.70354\n",
      "epoch: 07, loss: -0.70906\n",
      "epoch: 08, loss: -0.71353\n",
      "epoch: 09, loss: -0.68822\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 32/1000 [10:37<5:21:58, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "32 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0033.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0033\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.48257\n",
      "epoch: 01, loss: -0.63030\n",
      "epoch: 02, loss: -0.65909\n",
      "epoch: 03, loss: -0.67529\n",
      "epoch: 04, loss: -0.69236\n",
      "epoch: 05, loss: -0.69990\n",
      "epoch: 06, loss: -0.70702\n",
      "epoch: 07, loss: -0.70451\n",
      "epoch: 08, loss: -0.69655\n",
      "epoch: 09, loss: -0.71602\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 33/1000 [11:00<5:36:49, 20.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "33 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0034.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0034\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51205\n",
      "epoch: 01, loss: -0.64875\n",
      "epoch: 02, loss: -0.69123\n",
      "epoch: 03, loss: -0.69626\n",
      "epoch: 04, loss: -0.72191\n",
      "epoch: 05, loss: -0.71752\n",
      "epoch: 06, loss: -0.72839\n",
      "epoch: 07, loss: -0.73842\n",
      "epoch: 08, loss: -0.75333\n",
      "epoch: 09, loss: -0.75226\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34/1000 [11:22<5:41:43, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "34 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0035.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0035\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50267\n",
      "epoch: 01, loss: -0.68129\n",
      "epoch: 02, loss: -0.70084\n",
      "epoch: 03, loss: -0.70212\n",
      "epoch: 04, loss: -0.71223\n",
      "epoch: 05, loss: -0.73177\n",
      "epoch: 06, loss: -0.70314\n",
      "epoch: 07, loss: -0.72281\n",
      "epoch: 08, loss: -0.74291\n",
      "epoch: 09, loss: -0.73017\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 35/1000 [11:43<5:36:24, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "35 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0036.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0036\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53530\n",
      "epoch: 01, loss: -0.68523\n",
      "epoch: 02, loss: -0.70378\n",
      "epoch: 03, loss: -0.70605\n",
      "epoch: 04, loss: -0.72075\n",
      "epoch: 05, loss: -0.73545\n",
      "epoch: 06, loss: -0.74534\n",
      "epoch: 07, loss: -0.76294\n",
      "epoch: 08, loss: -0.75661\n",
      "epoch: 09, loss: -0.76315\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36/1000 [12:05<5:41:46, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "36 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0037.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0037\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.49829\n",
      "epoch: 01, loss: -0.65011\n",
      "epoch: 02, loss: -0.69042\n",
      "epoch: 03, loss: -0.70225\n",
      "epoch: 04, loss: -0.71774\n",
      "epoch: 05, loss: -0.73218\n",
      "epoch: 06, loss: -0.72753\n",
      "epoch: 07, loss: -0.72573\n",
      "epoch: 08, loss: -0.74035\n",
      "epoch: 09, loss: -0.72629\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 37/1000 [12:28<5:50:38, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "37 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0038.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0038\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.47156\n",
      "epoch: 01, loss: -0.62668\n",
      "epoch: 02, loss: -0.63069\n",
      "epoch: 03, loss: -0.63839\n",
      "epoch: 04, loss: -0.64794\n",
      "epoch: 05, loss: -0.65505\n",
      "epoch: 06, loss: -0.66141\n",
      "epoch: 07, loss: -0.66493\n",
      "epoch: 08, loss: -0.69467\n",
      "epoch: 09, loss: -0.69939\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38/1000 [12:50<5:49:01, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "38 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0039.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0039\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54774\n",
      "epoch: 01, loss: -0.73051\n",
      "epoch: 02, loss: -0.75211\n",
      "epoch: 03, loss: -0.76887\n",
      "epoch: 04, loss: -0.77999\n",
      "epoch: 05, loss: -0.79002\n",
      "epoch: 06, loss: -0.79552\n",
      "epoch: 07, loss: -0.79380\n",
      "epoch: 08, loss: -0.78665\n",
      "epoch: 09, loss: -0.79120\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 39/1000 [13:12<5:51:55, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "39 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0040.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0040\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.54918\n",
      "epoch: 01, loss: -0.70714\n",
      "epoch: 02, loss: -0.72229\n",
      "epoch: 03, loss: -0.73540\n",
      "epoch: 04, loss: -0.74503\n",
      "epoch: 05, loss: -0.74812\n",
      "epoch: 06, loss: -0.75649\n",
      "epoch: 07, loss: -0.76716\n",
      "epoch: 08, loss: -0.77771\n",
      "epoch: 09, loss: -0.77830\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 40/1000 [13:32<5:44:13, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "40 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0041.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0041\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.50067\n",
      "epoch: 01, loss: -0.65909\n",
      "epoch: 02, loss: -0.67713\n",
      "epoch: 03, loss: -0.69982\n",
      "epoch: 04, loss: -0.72672\n",
      "epoch: 05, loss: -0.72818\n",
      "epoch: 06, loss: -0.72904\n",
      "epoch: 07, loss: -0.72521\n",
      "epoch: 08, loss: -0.73544\n",
      "epoch: 09, loss: -0.73718\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [13:59<6:08:30, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "41 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0042.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0042\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.45041\n",
      "epoch: 01, loss: -0.64213\n",
      "epoch: 02, loss: -0.68193\n",
      "epoch: 03, loss: -0.69060\n",
      "epoch: 04, loss: -0.69156\n",
      "epoch: 05, loss: -0.68359\n",
      "epoch: 06, loss: -0.70614\n",
      "epoch: 07, loss: -0.69994\n",
      "epoch: 08, loss: -0.69553\n",
      "epoch: 09, loss: -0.70437\n",
      "After Unsqueezing, feature size= torch.Size([400, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42/1000 [14:17<5:45:54, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 400])\n",
      "42 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0043.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0043\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.58694\n",
      "epoch: 01, loss: -0.73780\n",
      "epoch: 02, loss: -0.76003\n",
      "epoch: 03, loss: -0.76608\n",
      "epoch: 04, loss: -0.77421\n",
      "epoch: 05, loss: -0.77168\n",
      "epoch: 06, loss: -0.77341\n",
      "epoch: 07, loss: -0.77946\n",
      "epoch: 08, loss: -0.78289\n",
      "epoch: 09, loss: -0.78537\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 43/1000 [14:37<5:36:24, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "43 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0044.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0044\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.51921\n",
      "epoch: 01, loss: -0.64734\n",
      "epoch: 02, loss: -0.66386\n",
      "epoch: 03, loss: -0.67561\n",
      "epoch: 04, loss: -0.67976\n",
      "epoch: 05, loss: -0.70035\n",
      "epoch: 06, loss: -0.71052\n",
      "epoch: 07, loss: -0.68691\n",
      "epoch: 08, loss: -0.67952\n",
      "epoch: 09, loss: -0.67900\n",
      "After Unsqueezing, feature size= torch.Size([525, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [15:04<6:04:43, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 525])\n",
      "44 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0045.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0045\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53488\n",
      "epoch: 01, loss: -0.70295\n",
      "epoch: 02, loss: -0.72813\n",
      "epoch: 03, loss: -0.74219\n",
      "epoch: 04, loss: -0.74942\n",
      "epoch: 05, loss: -0.75084\n",
      "epoch: 06, loss: -0.75517\n",
      "epoch: 07, loss: -0.75933\n",
      "epoch: 08, loss: -0.76404\n",
      "epoch: 09, loss: -0.77282\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 45/1000 [15:28<6:09:56, 23.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "45 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0046.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0046\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.53516\n",
      "epoch: 01, loss: -0.68026\n",
      "epoch: 02, loss: -0.69403\n",
      "epoch: 03, loss: -0.70328\n",
      "epoch: 04, loss: -0.71136\n",
      "epoch: 05, loss: -0.72626\n",
      "epoch: 06, loss: -0.73185\n",
      "epoch: 07, loss: -0.74305\n",
      "epoch: 08, loss: -0.73849\n",
      "epoch: 09, loss: -0.73465\n",
      "After Unsqueezing, feature size= torch.Size([450, 1, 1, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 46/1000 [15:52<6:11:09, 23.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([5]) eigenvectors shape torch.Size([5, 450])\n",
      "46 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0047.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "0047\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.52716\n",
      "epoch: 01, loss: -0.70347\n",
      "epoch: 02, loss: -0.73098\n",
      "epoch: 03, loss: -0.73700\n"
     ]
    }
   ],
   "source": [
    "utils.make_output_dir(output_dir)\n",
    "feat_list=[]\n",
    "inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "for inp in tqdm(inputs):\n",
    "    index, features_file = inp\n",
    "    print(index, features_file)\n",
    "     # Load\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    print(data_dict.keys())   #['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape']\n",
    "    # print(\"shape=\", data_dict['shape'], \"k shape\", data_dict['k'].shape, \"patch_size=\", data_dict['patch_size'])\n",
    "    image_id = data_dict['file'][:-4]\n",
    "    print(image_id)\n",
    "    # Load\n",
    "    output_file = str(Path(output_dir) / f'{image_id}.pth')\n",
    "    if Path(output_file).is_file():\n",
    "        print(f'Skipping existing file {str(output_file)}')\n",
    "        # break\n",
    "        # return  # skip because already generated\n",
    "\n",
    "    # Load affinity matrix\n",
    "    feats = data_dict[which_features].squeeze().cuda()\n",
    "    # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "    if normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "    # print(\"After normalization, Features Shape\",feats.shape)\n",
    "    # print(\"which_matrix=\", which_matrix)\n",
    "    # Eigenvectors of affinity matrix\n",
    "    if which_matrix == 'affinity_torch':\n",
    "        W = feats @ feats.T\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "            # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = torch.eig(W, eigenvectors=True)\n",
    "        eigenvalues = eigenvalues.cpu()\n",
    "        eigenvectors = eigenvectors.cpu()\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity_svd':\n",
    "        USV = torch.linalg.svd(feats, full_matrices=False)\n",
    "        eigenvectors = USV[0][:, :K].T.to('cpu', non_blocking=True)\n",
    "        eigenvalues = USV[1][:K].to('cpu', non_blocking=True)\n",
    "        print(\"which matrix=\",which_matrix,\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity':\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "        W = (feats @ feats.T)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "        W = W.cpu().numpy()\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = eigsh(W, which='LM', k=K)\n",
    "        eigenvectors = torch.flip(torch.from_numpy(eigenvectors), dims=(-1,)).T\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of matting laplacian matrix\n",
    "    elif which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "\n",
    "        # Get sizes\n",
    "        B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "        if image_downsample_factor is None:\n",
    "            image_downsample_factor = P\n",
    "        H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "\n",
    "        # Upscale features to match the resolution\n",
    "        if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "            feats = F.interpolate(\n",
    "                feats.T.reshape(1, -1, H_patch, W_patch),\n",
    "                size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "            ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "\n",
    "        ### Feature affinities\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "\n",
    "        W_feat_ds = (feats @ feats.T)\n",
    "        # feat_list.append(feats)\n",
    "        feat_dataset = Feature_Dataset(feats)\n",
    "        if feats.shape[0]%2==0:\n",
    "            features_dataloader = DataLoader(feat_dataset, batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            features_dataloader = DataLoader(feat_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        model_simsiam = SimSiam()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model_simsiam.to(device)\n",
    "        criterion = NegativeCosineSimilarity()\n",
    "        optimizer = torch.optim.SGD(model_simsiam.parameters(), lr=0.06)\n",
    "        print(\"Starting Training\")\n",
    "        for epoch in range(10):\n",
    "            total_loss = 0\n",
    "            for x0 in features_dataloader:\n",
    "            # for (x0), _, _ in features_dataloader:\n",
    "            #     print(\"Before Unsqueezing, x0 shape=\", x0.shape)\n",
    "                x0 = x0.unsqueeze(0).to(device)\n",
    "                x0 = x0.unsqueeze(1).to(device)\n",
    "                # print(\"After Unsqueezing x0 shape=\", x0.shape)\n",
    "                x1=torchvision.transforms.RandomAffine(0)(x0)\n",
    "                # print(\"After Unsqueezing x1 shape=\", x1.shape)\n",
    "                # x0 = x0.squeeze(0).to(device)\n",
    "                # print(\"batch_size=\",batch_size)\n",
    "                # x0_new = torch.tensor(x0).view(batch_size, 1, 1, 384)\n",
    "                # x0_new = torch.tensor(x0).view(batch_size, 1, 384)\n",
    "                x0_new = x0.view(batch_size, 1, 1, 384)\n",
    "                # print(\"After viewing x0 shape=\", x0_new.shape)\n",
    "                # print(\"x0.shape=\", x0.shape)\n",
    "                # x1 = x1.squeeze(0).to(device)\n",
    "                # x1 = torch.tensor(x1).view(batch_size, 1, 1,384)\n",
    "                x1_new = x1.view(batch_size, 1, 1, 384)\n",
    "                # print(\"After viewing x1 shape=\", x1_new.shape)\n",
    "                z0, p0 = model_simsiam(x0_new)\n",
    "                z1, p1 = model_simsiam(x1_new)\n",
    "                loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "                total_loss += loss.detach()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            avg_loss = total_loss / len(features_dataloader)\n",
    "            print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "        feats=feats.unsqueeze(1).to(device)\n",
    "        feats=feats.unsqueeze(2).to(device)\n",
    "        print(\"After Unsqueezing, feature size=\", feats.shape)\n",
    "        projected_feature=model_simsiam(feats)\n",
    "        W_feat_siam=torch.matmul(projected_feature[0], projected_feature[0].t())\n",
    "        W_feat=W_feat_ds + 0.1*W_feat_siam\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        if threshold_at_zero:\n",
    "            W_feat = (W_feat * (W_feat > 0))\n",
    "        W_feat = W_feat / W_feat.max()  # NOTE: If features are normalized, this naturally does nothing\n",
    "        # W_feat = W_feat.cpu().numpy()\n",
    "        W_feat = W_feat.detach().cpu().numpy()\n",
    "        # print(\"W_feat shape=\",W_feat.shape)\n",
    "\n",
    "        ### Color affinities\n",
    "        # If we are fusing with color affinites, then load the image and compute\n",
    "        if image_color_lambda > 0:\n",
    "\n",
    "            # Load image\n",
    "            image_file = str(Path(images_root) / f'{image_id}.jpg')\n",
    "            image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "            image_lr = np.array(image_lr) / 255.\n",
    "\n",
    "            # Color affinities (of type scipy.sparse.csr_matrix)\n",
    "            if which_color_matrix == 'knn':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "            elif which_color_matrix == 'rw':\n",
    "                W_lr = utils.rw_affinity(image_lr / 255)\n",
    "\n",
    "            # Convert to dense numpy array\n",
    "            W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "            # print(\"W_color shape\", W_color.shape)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # No color affinity\n",
    "            W_color = 0\n",
    "\n",
    "        # Combine\n",
    "        W_comb = W_feat + W_color * image_color_lambda  # combination\n",
    "        D_comb = np.array(utils.get_diagonal(W_comb).todense())  # is dense or sparse faster? not sure, should check\n",
    "        # print(\"W_comb shape= \", W_comb.shape, \"D_comb shape\",  D_comb.shape)\n",
    "        if lapnorm:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM', M=D_comb)\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM', M=D_comb)\n",
    "        else:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM')\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM')\n",
    "        eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "    print(\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "    # Sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "\n",
    "    # Save dict\n",
    "    output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "    torch.save(output_dict, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}