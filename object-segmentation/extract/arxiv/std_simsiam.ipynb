{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# # from functools import partial\n",
    "from pathlib import Path\n",
    "# from typing import Optional, Tuple\n",
    "# import cv2\n",
    "# import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "# from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "# from sklearn.decomposition import PCA\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "import extract_utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from argparse import Namespace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16\"\n",
    "inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "which_features='k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of inputs= 1000\n",
      "type of inputs[0] <class 'tuple'>\n",
      "length of inputs[0]= 2\n",
      "inputs[0][0]= 0\n",
      "inputs[0][1]= /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/dino_vits16/0001.pth\n"
     ]
    }
   ],
   "source": [
    "# print(\"type of inputs=\", type(inputs))\n",
    "print(\"len of inputs=\",len(inputs))\n",
    "print(\"type of inputs[0]\", type(inputs[0]))\n",
    "print(\"length of inputs[0]=\",len(inputs[0]))\n",
    "print(\"inputs[0][0]=\",inputs[0][0])\n",
    "print(\"inputs[0][1]=\", inputs[0][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "feat_list=[]\n",
    "for inp in tqdm(inputs[:1]):\n",
    "    index, features_file = inp\n",
    "    # print(index, features_file)\n",
    "     # Load\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    feats = data_dict[which_features].squeeze().cuda()\n",
    "    print(feats.shape)\n",
    "    # tensor_features=torch.stack(torch.tensor(feats))\n",
    "    feat_list.append(feats)\n",
    "    # print(\"data_dict.keys=\", data_dict.keys())\n",
    "    # print('data_dict[k]:', data_dict['k'])\n",
    "    # print('data_dict[k].shape=', data_dict['k'].shape)\n",
    "    # print('data_dict[indices]:', data_dict['indices'])\n",
    "    # print('data_dict[file]:', data_dict['file'])\n",
    "    # print('data_dict[model_name]:', data_dict['model_name'])\n",
    "    # print('data_dict[patch_size]:', data_dict['patch_size'])\n",
    "    # print('data_dict[shape]:', data_dict['shape'])\n",
    "    # print('type of data_dict[shape].shape=', type(data_dict['shape']))\n",
    "    # print('data_dict[shape][0]:', data_dict['shape'][0])\n",
    "    # print('data_dict[shape][1]:', data_dict['shape'][1])\n",
    "    # print('data_dict[shape][2]:', data_dict['shape'][2])\n",
    "    # print('data_dict[shape][3]:', data_dict['shape'][3])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "training_args={\n",
    "    'arch':\"resnet50\",\n",
    "    'dim':2048,\n",
    "    'pred_dim':512,\n",
    "    'fix_pred_lr':'store_true',\n",
    "    'momentum':0.9,\n",
    "    'wd': 1e-4,\n",
    "    'lr':0.05,\n",
    "    'batch_size':8,\n",
    "    'data': \"/home/phdcs2/Hard_Disk/Projects/Codes/lightly/examples/pytorch/datasets/cifar10\",\n",
    "    'workers':32,\n",
    "    'start_epoch':0,\n",
    "    'epochs':10\n",
    "}\n",
    "args = Namespace(**training_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Dataset on FeatureSpace"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([400, 384])\n"
     ]
    }
   ],
   "source": [
    "print(len(feat_list))\n",
    "print(type(feat_list[0]))\n",
    "print(feat_list[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Feature_Dataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# features=data_dict['k']\n",
    "# print(features.shape)\n",
    "feat_dataset = Feature_Dataset(feat_list[:1])\n",
    "features_dataloader = DataLoader(feat_dataset, batch_size=1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dim=128\n",
    "        self.pred_dim=feats.shape[1]\n",
    "        self.projection_head = nn.Sequential(nn.Linear(feats.shape[1], 128, bias=False),\n",
    "                                        nn.BatchNorm1d(128),\n",
    "                                        nn.ReLU(inplace=True), # first layer\n",
    "                                        nn.Linear(128, self.pred_dim, bias=False),\n",
    "                                        nn.BatchNorm1d(self.pred_dim),\n",
    "                                        nn.ReLU(inplace=True), # second layer\n",
    "                                        # self.encoder.fc,\n",
    "                                        nn.BatchNorm1d(self.pred_dim, affine=False)) # output layer\n",
    "        self.prediction_head = nn.Sequential(nn.Linear(self.pred_dim, self.dim, bias=False),\n",
    "                                        nn.BatchNorm1d(self.dim),\n",
    "                                        nn.ReLU(inplace=True), # hidden layer\n",
    "                                        nn.Linear(self.dim, self.pred_dim,)) # output layer\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            x1: first views of images\n",
    "            x2: second views of images\n",
    "        Output:\n",
    "            p1, p2, z1, z2: predictors and targets of the network\n",
    "            See Sec. 3 of https://arxiv.org/abs/2011.10566 for detailed notations\n",
    "        \"\"\"\n",
    "\n",
    "        # compute features for one view\n",
    "        z1 = self.projection_head(x1) # NxC\n",
    "        z2 = self.projection_head(x2) # NxC\n",
    "\n",
    "        p1 = self.prediction_head(z1) # NxC\n",
    "        p2 = self.prediction_head(z2) # NxC\n",
    "\n",
    "        return p1, p2, z1.detach(), z2.detach()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "SimSiam(\n  (projection_head): Sequential(\n    (0): Linear(in_features=384, out_features=128, bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=128, out_features=384, bias=False)\n    (4): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): ReLU(inplace=True)\n    (6): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n  )\n  (prediction_head): Sequential(\n    (0): Linear(in_features=384, out_features=128, bias=False)\n    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): Linear(in_features=128, out_features=384, bias=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simsiam = SimSiam()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_simsiam.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "criterion = nn.CosineSimilarity(dim=1).cuda(0)\n",
    "optimizer = torch.optim.SGD(model_simsiam.parameters(), lr=0.06,momentum=args.momentum,\n",
    "                                weight_decay=args.wd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 00, loss: 0.00579\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 01, loss: -0.01513\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 02, loss: -0.04892\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 03, loss: -0.08325\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 04, loss: -0.11363\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 05, loss: -0.13930\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 06, loss: -0.16205\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 07, loss: -0.18320\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 08, loss: -0.20398\n",
      "p1.shape= torch.Size([400, 384])\n",
      "p2.shape= torch.Size([400, 384])\n",
      "z1.shape= torch.Size([400, 384])\n",
      "z2.shape= torch.Size([400, 384])\n",
      "epoch: 09, loss: -0.22555\n"
     ]
    }
   ],
   "source": [
    "# print(\"Starting Training\")\n",
    "losses = AverageMeter('Loss', ':.4f')\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for x0 in features_dataloader:\n",
    "    # for (x0), _, _ in features_dataloader:\n",
    "        x1=torchvision.transforms.RandomAffine(0)(x0)\n",
    "        x0 = x0.squeeze().to(device)\n",
    "        # x0 = x0.to(device)\n",
    "        # print(\"x0.shape=\", x0.shape)\n",
    "        x1 = x1.squeeze().to(device)\n",
    "        # print(\"x1 shape=\", x1.shape)\n",
    "        p1, p2, z1, z2 = model_simsiam(x1=x0, x2=x1)\n",
    "        print(\"p1.shape=\", p1.shape)\n",
    "        print(\"p2.shape=\", p2.shape)\n",
    "        print(\"z1.shape=\", z1.shape)\n",
    "        print(\"z2.shape=\", z2.shape)\n",
    "        loss = -(criterion(p1, z2).mean() + criterion(p2, z1).mean()) * 0.5\n",
    "        total_loss += loss.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # losses.update(loss.item(), x0.size(0))\n",
    "        #\n",
    "        # # compute gradient and do SGD step\n",
    "        # optimizer.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "    avg_loss = total_loss / len(features_dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}