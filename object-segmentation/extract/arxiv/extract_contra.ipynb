{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import cv2\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "import extract_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/phdcs2/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch size= 16 number of heads= 6\n"
     ]
    }
   ],
   "source": [
    "images_list=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/lists/images.txt\"\n",
    "images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/images/trainval/JPEGImages\"\n",
    "model_name=\"dino_vits16\"\n",
    "batch_size=1\n",
    "output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\"\n",
    "which_block=-1\n",
    "# Output\n",
    "utils.make_output_dir(output_dir)\n",
    "# Models\n",
    "model_name = model_name.lower()\n",
    "model, val_transform, patch_size, num_heads = utils.get_model(model_name)    #patch size= 16 number of heads= 6\n",
    "print(\"patch size=\", patch_size, \"number of heads=\", num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add hook\n",
    "if 'dino' in model_name or 'mocov3' in model_name:\n",
    "    feat_out = {}\n",
    "    def hook_fn_forward_qkv(module, input, output):\n",
    "        print(\"feat_out.keys()\", feat_out.keys())\n",
    "        feat_out[\"qkv\"] = output\n",
    "    model._modules[\"blocks\"][which_block]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "else:\n",
    "    raise ValueError(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: len(dataset)=17125\n",
      "Dataloader size: len(dataloader)=17125\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "filenames = Path(images_list).read_text().splitlines()\n",
    "dataset = utils.ImagesDataset(filenames=filenames, images_root=images_root, transform=val_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8)\n",
    "print(f'Dataset size: {len(dataset)=}')\n",
    "print(f'Dataloader size: {len(dataloader)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.num_features= 384\n"
     ]
    }
   ],
   "source": [
    "# Prepare\n",
    "# accelerator = Accelerator(fp16=True, cpu=False)\n",
    "accelerator = Accelerator(cpu=False)\n",
    "# model, dataloader = accelerator.prepare(model, dataloader)\n",
    "model = model.to(accelerator.device)\n",
    "model.num_features\n",
    "print(\"model.num_features=\", model.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method Module.get_submodule of VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0-11): 12 x Block(\n      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=384, out_features=384, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): Identity()\n      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n  (head): Identity()\n  (fc): Identity()\n)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 17125/17125 [00:20<00:00, 817.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process\n",
    "pbar = list(tqdm(dataloader, desc='Processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_out.keys()= dict_keys([])\n",
      "images.shape= torch.Size([1, 3, 500, 486]) files = ('2007_000027.jpg',) indices tensor([0])\n",
      "id= 2007_000027\n",
      "images.shape after padding= torch.Size([1, 3, 496, 480])\n",
      "feat_out.keys() dict_keys([])\n",
      "feat_out.keys() dict_keys(['qkv'])\n",
      "tensor([[[ 3.5655, -2.2402, -1.6416,  ...,  1.1889,  7.2091,  2.7513],\n",
      "         [ 4.4418,  3.3791,  0.7386,  ...,  1.1828, -8.3420, -4.6585],\n",
      "         [ 2.0956,  1.9108,  1.4090,  ...,  1.0758, -7.9683, -4.6065],\n",
      "         ...,\n",
      "         [ 3.4554, -4.3746,  1.3775,  ..., -2.6455,  5.0318, -9.4393],\n",
      "         [ 0.7387,  0.7254,  0.6072,  ...,  0.5993,  6.5360, -7.0040],\n",
      "         [ 1.3426, -3.9547,  1.7055,  ...,  2.1555,  3.1264, -6.6079]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "feat_out.keys()= dict_keys(['qkv'])\n",
      "type(output_qkv) <class 'torch.Tensor'>\n",
      "output_qkv.shape torch.Size([3, 1, 6, 931, 64])\n",
      "output_qkv[0].shape torch.Size([1, 6, 931, 64])\n",
      "output_qkv[1].shape torch.Size([1, 6, 931, 64])\n",
      "output_qkv[2].shape torch.Size([1, 6, 931, 64])\n",
      "output_dict[k].shape= torch.Size([1, 930, 384])\n",
      "output_dict.keys() dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "output_dict['k'].shape= torch.Size([1, 930, 384]) output_dict['indices'] = tensor(0) output_dict['file'] = 2007_000027.jpg output_dict['id']= 2007_000027 output_dict['model_name'] = dino_vits16  output_dict['shape'] =( 1 3 500 486 )output_dict['patch_size'] = 16\n",
      "images.shape= torch.Size([1, 3, 281, 500]) files = ('2007_000032.jpg',) indices tensor([1])\n",
      "id= 2007_000032\n",
      "images.shape after padding= torch.Size([1, 3, 272, 496])\n",
      "feat_out.keys() dict_keys(['qkv'])\n",
      "feat_out.keys() dict_keys(['qkv'])\n",
      "tensor([[[ -1.9172,   4.7719,   0.2689,  ..., -13.1136,   4.0221,  -5.2282],\n",
      "         [ -0.4135,   0.4951,   0.9256,  ...,  -0.8270,  -2.1598,  -5.3021],\n",
      "         [ -0.6958,   0.4790,   0.6134,  ...,  -2.6062,  -2.4562,  -4.7065],\n",
      "         ...,\n",
      "         [ -5.8406,  -2.6643,  -5.6756,  ...,  -5.3330,   1.4399,   2.4445],\n",
      "         [ -5.3045,  -4.0285,  -3.7538,  ...,  -3.9518,   2.2268,   5.2006],\n",
      "         [ -4.1165,   2.5278,  -3.6340,  ...,   0.2152,  -1.0075,   8.1371]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "feat_out.keys()= dict_keys(['qkv'])\n",
      "type(output_qkv) <class 'torch.Tensor'>\n",
      "output_qkv.shape torch.Size([3, 1, 6, 528, 64])\n",
      "output_qkv[0].shape torch.Size([1, 6, 528, 64])\n",
      "output_qkv[1].shape torch.Size([1, 6, 528, 64])\n",
      "output_qkv[2].shape torch.Size([1, 6, 528, 64])\n",
      "output_dict[k].shape= torch.Size([1, 527, 384])\n",
      "output_dict.keys() dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "output_dict['k'].shape= torch.Size([1, 527, 384]) output_dict['indices'] = tensor(1) output_dict['file'] = 2007_000032.jpg output_dict['id']= 2007_000032 output_dict['model_name'] = dino_vits16  output_dict['shape'] =( 1 3 281 500 )output_dict['patch_size'] = 16\n",
      "Saved features to /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\n"
     ]
    }
   ],
   "source": [
    "# Process\n",
    "# pbar = list(tqdm(dataloader, desc='Processing'))\n",
    "print(\"feat_out.keys()=\", feat_out.keys())\n",
    "for i, (images, files, indices) in enumerate(pbar[:2]):\n",
    "    output_dict = {}\n",
    "    print(\"images.shape=\", images.shape, \"files =\", files, \"indices\", indices)\n",
    "\n",
    "    # Check if file already exists\n",
    "    id = Path(files[0]).stem\n",
    "    print(\"id=\", id)\n",
    "    output_file = Path(output_dir) / f'{id}.pth'\n",
    "    # if output_file.is_file():\n",
    "    #     pbar.write(f'Skipping existing file {str(output_file)}')\n",
    "    #     continue\n",
    "\n",
    "    # Reshape image\n",
    "    P = patch_size\n",
    "    B, C, H, W = images.shape\n",
    "    H_patch, W_patch = H // P, W // P\n",
    "    H_pad, W_pad = H_patch * P, W_patch * P\n",
    "    T = H_patch * W_patch + 1  # number of tokens, add 1 for [CLS]\n",
    "    # images = F.interpolate(images, size=(H_pad, W_pad), mode='bilinear')  # resize image\n",
    "    images = images[:, :, :H_pad, :W_pad]\n",
    "    images = images.to(accelerator.device)\n",
    "    print(\"images.shape after padding=\", images.shape)\n",
    "\n",
    "    # Forward and collect features into output dict\n",
    "    if 'dino' in model_name or 'mocov3' in model_name:\n",
    "        # accelerator.unwrap_model(model).get_intermediate_layers(images)[0].squeeze(0)\n",
    "        model.get_intermediate_layers(images)[0].squeeze(0)\n",
    "        print(model.get_intermediate_layers(images)[0])\n",
    "        # output_dict['out'] = out\n",
    "        print(\"feat_out.keys()=\", feat_out.keys())\n",
    "        output_qkv = feat_out[\"qkv\"].reshape(B, T, 3, num_heads, -1 // num_heads).permute(2, 0, 3, 1, 4)\n",
    "        print(\"type(output_qkv)\", type(output_qkv))\n",
    "        print(\"output_qkv.shape\", output_qkv.shape)    #3, 1, 6, 931, 64]\n",
    "        print(\"output_qkv[0].shape\", output_qkv[0].shape)\n",
    "        print(\"output_qkv[1].shape\", output_qkv[1].shape)\n",
    "        print(\"output_qkv[2].shape\", output_qkv[2].shape)\n",
    "        # output_dict['q'] = output_qkv[0].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "        output_dict['k'] = output_qkv[1].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "        print(\"output_dict[k].shape=\", output_dict['k'].shape)\n",
    "        # output_dict['v'] = output_qkv[2].transpose(1, 2).reshape(B, T, -1)[:, 1:, :]\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "\n",
    "    # print(\"output_dict.items=\", output_dict.items())\n",
    "    # Metadata\n",
    "    output_dict['indices'] = indices[0]\n",
    "    output_dict['file'] = files[0]\n",
    "    output_dict['id'] = id\n",
    "    output_dict['model_name'] = model_name\n",
    "    output_dict['patch_size'] = patch_size\n",
    "    output_dict['shape'] = (B, C, H, W)\n",
    "    output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "    # for k, v in output_dict.items():\n",
    "    #     print(\"k=\", k)\n",
    "    #     if torch.is_tensor(v):\n",
    "    #         print(\"v.shape\", v.shape)\n",
    "    #     else:\n",
    "    #         print(\"v=\", v)\n",
    "    print(\"output_dict.keys()\", output_dict.keys())\n",
    "    print(\"output_dict['k'].shape=\", output_dict['k'].shape,\"output_dict['indices'] =\", indices[0],\"output_dict['file'] =\", files[0],\"output_dict['id']=\" , id, \"output_dict['model_name'] =\", model_name,\" output_dict['shape'] =(\", B, C, H, W,\")output_dict['patch_size'] =\",  patch_size )\n",
    "    # Save\n",
    "    accelerator.save(output_dict, str(output_file))\n",
    "    accelerator.wait_for_everyone()\n",
    "print(f'Saved features to {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# output_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v) for k, v in output_dict.items()}\n",
    "# print(output_dict.keys())\n",
    "# for k, v in output_dict.items():\n",
    "#     print(\"k=\", k)\n",
    "#     print(\"v.shape=\", v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/images/trainval/JPEGImages\"\n",
    "features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\"\n",
    "output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_contrastive\"\n",
    "which_matrix= 'laplacian'\n",
    "which_color_matrix= 'knn'\n",
    "which_features= 'k'\n",
    "normalize=True\n",
    "threshold_at_zero=True\n",
    "lapnorm= True\n",
    "K= 20\n",
    "image_downsample_factor = None\n",
    "image_color_lambda = 0.0\n",
    "multiprocessing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "def log(x):\n",
    "    return torch.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def contrastive_affinity(matrix1, matrix2):\n",
    "    mat_mul=matrix1 @ matrix2\n",
    "    print(\"mat_mul device\", mat_mul.device)\n",
    "\n",
    "    #Diagonal elements\n",
    "    if torch.cuda.is_available():\n",
    "        mask=torch.eye(mat_mul.shape[0], dtype=bool).cuda()\n",
    "    print(\"mask device\", mask.device)\n",
    "    diag=mask\n",
    "    # print(diag)\n",
    "    diag_val=-log(sigmoid(mat_mul[diag]))\n",
    "    print(\"diag_val device\", diag_val.device)\n",
    "    # print(diag_val)\n",
    "\n",
    "    # Off-Diagonal Elements\n",
    "    off_diag=~mask\n",
    "    # print(off_diag)\n",
    "    off_diag_val=-log(1-sigmoid(mat_mul[off_diag]))\n",
    "    # print(off_diag_val)\n",
    "    print(\"off-diag device\",off_diag_val.device)\n",
    "\n",
    "    # Combining Diagonal and off-Diagonal element for contrastive affinities\n",
    "    if torch.cuda.is_available():\n",
    "        contra_affinity = torch.zeros(matrix1.shape[0],matrix2.shape[1]).cuda()\n",
    "    print(\"contra affinity device\", contra_affinity.device)\n",
    "    # print(contra_affinity.shape)\n",
    "    torch.diagonal(contra_affinity).copy_(diag_val)\n",
    "    contra_affinity.masked_scatter_(~torch.eye(matrix1.shape[0], dtype=torch.bool).cuda(), off_diag_val)\n",
    "    return contra_affinity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_contrastive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16/2007_000027.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "shape= (1, 3, 500, 486) k shape torch.Size([1, 930, 384]) patch_size= 16\n",
      "2007_000027\n",
      "Skipping existing file /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_contrastive/2007_000027.pth\n",
      "Without normalizing, Features Shape is torch.Size([930, 384])\n",
      "After normalization, Features Shape torch.Size([930, 384])\n",
      "which_matrix= laplacian\n",
      "Without normalizing, Features Shape is torch.Size([930, 384])\n",
      "mat_mul device cuda:0\n",
      "mask device cuda:0\n",
      "diag_val device cuda:0\n",
      "off-diag device cuda:0\n",
      "contra affinity device cuda:0\n",
      "W_feat shape= (930, 930)\n",
      "W_comb shape=  (930, 930) D_comb shape (930, 930)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([20]) eigenvectors shape torch.Size([20, 930])\n",
      "1 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16/2007_000032.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "shape= (1, 3, 281, 500) k shape torch.Size([1, 527, 384]) patch_size= 16\n",
      "2007_000032\n",
      "Skipping existing file /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_contrastive/2007_000032.pth\n",
      "Without normalizing, Features Shape is torch.Size([527, 384])\n",
      "After normalization, Features Shape torch.Size([527, 384])\n",
      "which_matrix= laplacian\n",
      "Without normalizing, Features Shape is torch.Size([527, 384])\n",
      "mat_mul device cuda:0\n",
      "mask device cuda:0\n",
      "diag_val device cuda:0\n",
      "off-diag device cuda:0\n",
      "contra affinity device cuda:0\n",
      "W_feat shape= (527, 527)\n",
      "W_comb shape=  (527, 527) D_comb shape (527, 527)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([20]) eigenvectors shape torch.Size([20, 527])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "utils.make_output_dir(output_dir)\n",
    "inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "for inp in tqdm(inputs[:2]):\n",
    "    index, features_file = inp\n",
    "    print(index, features_file)\n",
    "     # Load\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    print(data_dict.keys())   #['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape']\n",
    "    print(\"shape=\", data_dict['shape'], \"k shape\", data_dict['k'].shape, \"patch_size=\", data_dict['patch_size'])\n",
    "    image_id = data_dict['file'][:-4]\n",
    "    print(image_id)\n",
    "    # Load\n",
    "    output_file = str(Path(output_dir) / f'{image_id}.pth')\n",
    "    if Path(output_file).is_file():\n",
    "        print(f'Skipping existing file {str(output_file)}')\n",
    "        # break\n",
    "        # return  # skip because already generated\n",
    "\n",
    "    # Load affinity matrix\n",
    "    feats = data_dict[which_features].squeeze().cuda()\n",
    "    print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "    if normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "    print(\"After normalization, Features Shape\",feats.shape)\n",
    "    print(\"which_matrix=\", which_matrix)\n",
    "    # Eigenvectors of affinity matrix\n",
    "    if which_matrix == 'affinity_torch':\n",
    "        # W = feats @ feats.T\n",
    "        W_feat=contrastive_affinity(feats, feats.T)\n",
    "        print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "            print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = torch.eig(W, eigenvectors=True)\n",
    "        eigenvalues = eigenvalues.cpu()\n",
    "        eigenvectors = eigenvectors.cpu()\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity_svd':\n",
    "        USV = torch.linalg.svd(feats, full_matrices=False)\n",
    "        eigenvectors = USV[0][:, :K].T.to('cpu', non_blocking=True)\n",
    "        eigenvalues = USV[1][:K].to('cpu', non_blocking=True)\n",
    "        print(\"which matrix=\",which_matrix,\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity':\n",
    "        print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "        # W = (feats @ feats.T)\n",
    "        W_feat=contrastive_affinity(feats, feats.T)\n",
    "        print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "        W = W.cpu().numpy()\n",
    "        print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = eigsh(W, which='LM', k=K)\n",
    "        eigenvectors = torch.flip(torch.from_numpy(eigenvectors), dims=(-1,)).T\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of matting laplacian matrix\n",
    "    elif which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "\n",
    "        # Get sizes\n",
    "        B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "        if image_downsample_factor is None:\n",
    "            image_downsample_factor = P\n",
    "        H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "\n",
    "        # Upscale features to match the resolution\n",
    "        if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "            feats = F.interpolate(\n",
    "                feats.T.reshape(1, -1, H_patch, W_patch),\n",
    "                size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "            ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "\n",
    "        ### Feature affinities\n",
    "        print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "        # W_feat = (feats @ feats.T)\n",
    "        W_feat=contrastive_affinity(feats, feats.T)\n",
    "        if threshold_at_zero:\n",
    "            W_feat = (W_feat * (W_feat > 0))\n",
    "        W_feat = W_feat / W_feat.max()  # NOTE: If features are normalized, this naturally does nothing\n",
    "        W_feat = W_feat.cpu().numpy()\n",
    "        print(\"W_feat shape=\",W_feat.shape)\n",
    "\n",
    "        ### Color affinities\n",
    "        # If we are fusing with color affinites, then load the image and compute\n",
    "        if image_color_lambda > 0:\n",
    "\n",
    "            # Load image\n",
    "            image_file = str(Path(images_root) / f'{image_id}.jpg')\n",
    "            image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "            image_lr = np.array(image_lr) / 255.\n",
    "\n",
    "            # Color affinities (of type scipy.sparse.csr_matrix)\n",
    "            if which_color_matrix == 'knn':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "            elif which_color_matrix == 'rw':\n",
    "                W_lr = utils.rw_affinity(image_lr / 255)\n",
    "\n",
    "            # Convert to dense numpy array\n",
    "            W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "            print(\"W_color shape\", W_color.shape)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # No color affinity\n",
    "            W_color = 0\n",
    "\n",
    "        # Combine\n",
    "        W_comb = W_feat + W_color * image_color_lambda  # combination\n",
    "        D_comb = np.array(utils.get_diagonal(W_comb).todense())  # is dense or sparse faster? not sure, should check\n",
    "        print(\"W_comb shape= \", W_comb.shape, \"D_comb shape\",  D_comb.shape)\n",
    "\n",
    "        # Extract eigenvectors\n",
    "        if lapnorm:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM', M=D_comb)\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM', M=D_comb)\n",
    "        else:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM')\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM')\n",
    "        eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "    print(\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "    # Sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "\n",
    "    # Save dict\n",
    "    output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "    torch.save(output_dict, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}