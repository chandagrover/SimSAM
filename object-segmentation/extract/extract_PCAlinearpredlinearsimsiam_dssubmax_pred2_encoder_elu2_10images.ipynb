{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "from pathlib import Path\n",
    "# from typing import Optional, Tuple\n",
    "# import cv2\n",
    "# import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from scipy.sparse.linalg import eigsh\n",
    "# from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "# import extract_utils_dinov2 as utils\n",
    "import extract_utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from scipy.ndimage import affine_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Extract Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/images\"\n",
    "# # features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD_1/features/dino_vits16\"\n",
    "# # features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/\"\n",
    "# features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features_dinov2/\"\n",
    "# output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/eigs_dot1PCA64linearpredlinear_dssubmax_pred2_encoder_elu_corr2_dinov2_vitg14\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/images/trainval/JPEGImages\"\n",
    "# features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD_1/features/dino_vits16\"\n",
    "# features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/\"\n",
    "features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16\"\n",
    "output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/eigs_ours_model\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/images/trainval/JPEGImages\"\n",
    "# # features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD_1/features/dino_vits16\"\n",
    "# # features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD/features/\"\n",
    "# features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\"\n",
    "# output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_ours_model\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# images_root=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/images/trainval/JPEGImages\"\n",
    "# # features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/object-segmentation/ECSSD_1/features/dino_vits16\"\n",
    "# features_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/features/dino_vits16\"\n",
    "# output_dir=\"/home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012/eigs_test/laplacian\"\n",
    "which_matrix= 'laplacian'\n",
    "which_color_matrix= 'knn'\n",
    "which_features= 'k'\n",
    "normalize=True\n",
    "threshold_at_zero=True\n",
    "lapnorm= True\n",
    "K=15\n",
    "image_downsample_factor = None\n",
    "image_color_lambda = 0.0\n",
    "multiprocessing = 0\n",
    "batch_size=2\n",
    "epochs=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Incorporating SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class Feature_Dataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# class SimSiam(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.projection_head = SimSiamProjectionHead(feats.shape[1], 128,feats.shape[1])\n",
    "#         self.prediction_head = SimSiamPredictionHead(feats.shape[1], 128, feats.shape[1])\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         z = self.projection_head(x)\n",
    "#         p = self.prediction_head(z)\n",
    "#         z = z.detach()\n",
    "#         return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def normalize_affinity_matrix(affinity_matrix, axis=0):\n",
    "#     if axis == 1:\n",
    "#         # Normalize by row-wise sums\n",
    "#         row_sums = torch.sum(affinity_matrix, dim=1, keepdim=True)\n",
    "#         print(row_sums)\n",
    "#         normalized_matrix = affinity_matrix / row_sums\n",
    "#     elif axis == 0:\n",
    "#         # Normalize by column-wise sums\n",
    "#         col_sums = torch.sum(affinity_matrix, dim=0, keepdim=True)\n",
    "#         normalized_matrix = affinity_matrix / col_sums\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid axis. Axis must be either 0 or 1.\")\n",
    "\n",
    "#     return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000027.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.92531\n",
      "epoch: 01, loss: -0.95557\n",
      "epoch: 02, loss: -0.96251\n",
      "epoch: 03, loss: -0.96660\n",
      "epoch: 04, loss: -0.96936\n",
      "epoch: 05, loss: -0.97144\n",
      "epoch: 06, loss: -0.97307\n",
      "epoch: 07, loss: -0.97437\n",
      "epoch: 08, loss: -0.97548\n",
      "epoch: 09, loss: -0.97642\n",
      "torch.Size([930, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:27<05:32, 27.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 930])\n",
      "1 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000032.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.89496\n",
      "epoch: 01, loss: -0.93679\n",
      "epoch: 02, loss: -0.94564\n",
      "epoch: 03, loss: -0.95101\n",
      "epoch: 04, loss: -0.95471\n",
      "epoch: 05, loss: -0.95757\n",
      "epoch: 06, loss: -0.95974\n",
      "epoch: 07, loss: -0.96162\n",
      "epoch: 08, loss: -0.96318\n",
      "epoch: 09, loss: -0.96444\n",
      "torch.Size([527, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [00:48<04:19, 23.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 527])\n",
      "2 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000033.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.90406\n",
      "epoch: 01, loss: -0.94692\n",
      "epoch: 02, loss: -0.95505\n",
      "epoch: 03, loss: -0.95980\n",
      "epoch: 04, loss: -0.96301\n",
      "epoch: 05, loss: -0.96545\n",
      "epoch: 06, loss: -0.96736\n",
      "epoch: 07, loss: -0.96893\n",
      "epoch: 08, loss: -0.97024\n",
      "epoch: 09, loss: -0.97133\n",
      "torch.Size([682, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [01:17<04:21, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 682])\n",
      "3 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000039.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000039\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.81977\n",
      "epoch: 01, loss: -0.89338\n",
      "epoch: 02, loss: -0.91289\n",
      "epoch: 03, loss: -0.92401\n",
      "epoch: 04, loss: -0.93132\n",
      "epoch: 05, loss: -0.93673\n",
      "epoch: 06, loss: -0.94067\n",
      "epoch: 07, loss: -0.94433\n",
      "epoch: 08, loss: -0.94703\n",
      "epoch: 09, loss: -0.94938\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [01:47<04:08, 27.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "4 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000042.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.88223\n",
      "epoch: 01, loss: -0.92720\n",
      "epoch: 02, loss: -0.93827\n",
      "epoch: 03, loss: -0.94492\n",
      "epoch: 04, loss: -0.94945\n",
      "epoch: 05, loss: -0.95288\n",
      "epoch: 06, loss: -0.95558\n",
      "epoch: 07, loss: -0.95778\n",
      "epoch: 08, loss: -0.95968\n",
      "epoch: 09, loss: -0.96121\n",
      "torch.Size([620, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [02:09<03:25, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 620])\n",
      "5 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000061.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000061\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.83660\n",
      "epoch: 01, loss: -0.90429\n",
      "epoch: 02, loss: -0.92135\n",
      "epoch: 03, loss: -0.93067\n",
      "epoch: 04, loss: -0.93721\n",
      "epoch: 05, loss: -0.94193\n",
      "epoch: 06, loss: -0.94548\n",
      "epoch: 07, loss: -0.94849\n",
      "epoch: 08, loss: -0.95079\n",
      "epoch: 09, loss: -0.95273\n",
      "torch.Size([620, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [02:23<02:30, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 620])\n",
      "6 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000063.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000063\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.81853\n",
      "epoch: 01, loss: -0.88725\n",
      "epoch: 02, loss: -0.90650\n",
      "epoch: 03, loss: -0.91764\n",
      "epoch: 04, loss: -0.92529\n",
      "epoch: 05, loss: -0.93095\n",
      "epoch: 06, loss: -0.93547\n",
      "epoch: 07, loss: -0.93916\n",
      "epoch: 08, loss: -0.94214\n",
      "epoch: 09, loss: -0.94456\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [02:38<01:57, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "7 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000068.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000068\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.90520\n",
      "epoch: 01, loss: -0.94453\n",
      "epoch: 02, loss: -0.95357\n",
      "epoch: 03, loss: -0.95883\n",
      "epoch: 04, loss: -0.96237\n",
      "epoch: 05, loss: -0.96496\n",
      "epoch: 06, loss: -0.96699\n",
      "epoch: 07, loss: -0.96869\n",
      "epoch: 08, loss: -0.97011\n",
      "epoch: 09, loss: -0.97132\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [02:56<01:34, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "8 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000121.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000121\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.83129\n",
      "epoch: 01, loss: -0.89830\n",
      "epoch: 02, loss: -0.91582\n",
      "epoch: 03, loss: -0.92591\n",
      "epoch: 04, loss: -0.93274\n",
      "epoch: 05, loss: -0.93776\n",
      "epoch: 06, loss: -0.94166\n",
      "epoch: 07, loss: -0.94481\n",
      "epoch: 08, loss: -0.94738\n",
      "epoch: 09, loss: -0.94963\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [03:15<01:16, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "9 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_000123.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_000123\n",
      "Starting Training\n",
      "epoch: 00, loss: -0.84419\n",
      "epoch: 01, loss: -0.90873\n",
      "epoch: 02, loss: -0.92513\n",
      "epoch: 03, loss: -0.93455\n",
      "epoch: 04, loss: -0.94053\n",
      "epoch: 05, loss: -0.94522\n",
      "epoch: 06, loss: -0.94871\n",
      "epoch: 07, loss: -0.95147\n",
      "epoch: 08, loss: -0.95390\n",
      "epoch: 09, loss: -0.95593\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [03:47<01:09, 23.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "10 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_001763.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_001763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.89802\n",
      "epoch: 01, loss: -0.93650\n",
      "epoch: 02, loss: -0.94618\n",
      "epoch: 03, loss: -0.95189\n",
      "epoch: 04, loss: -0.95571\n",
      "epoch: 05, loss: -0.95861\n",
      "epoch: 06, loss: -0.96091\n",
      "epoch: 07, loss: -0.96281\n",
      "epoch: 08, loss: -0.96436\n",
      "epoch: 09, loss: -0.96568\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [04:14<00:48, 24.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "11 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_001884.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_001884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.91466\n",
      "epoch: 01, loss: -0.95067\n",
      "epoch: 02, loss: -0.95838\n",
      "epoch: 03, loss: -0.96279\n",
      "epoch: 04, loss: -0.96585\n",
      "epoch: 05, loss: -0.96811\n",
      "epoch: 06, loss: -0.96987\n",
      "epoch: 07, loss: -0.97133\n",
      "epoch: 08, loss: -0.97251\n",
      "epoch: 09, loss: -0.97351\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [04:30<00:21, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n",
      "12 /home/phdcs2/Hard_Disk/Datasets/Deep-Spectral-Segmentation/data/VOC2012_10/features/dino_vits16/2007_004275.pth\n",
      "dict_keys(['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape'])\n",
      "2007_004275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phdcs2/Hard_Disk/Projects/T2I/deep-spectral-segmentation/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:642: RuntimeWarning: invalid value encountered in divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: -0.90641\n",
      "epoch: 01, loss: -0.94239\n",
      "epoch: 02, loss: -0.95106\n",
      "epoch: 03, loss: -0.95629\n",
      "epoch: 04, loss: -0.95977\n",
      "epoch: 05, loss: -0.96245\n",
      "epoch: 06, loss: -0.96458\n",
      "epoch: 07, loss: -0.96632\n",
      "epoch: 08, loss: -0.96770\n",
      "epoch: 09, loss: -0.96899\n",
      "torch.Size([713, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [04:47<00:00, 22.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues shape torch.Size([15]) eigenvectors shape torch.Size([15, 713])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pca_comp=64\n",
    "pca = PCA(n_components=pca_comp)\n",
    "utils.make_output_dir(output_dir)\n",
    "inputs = list(enumerate(sorted(Path(features_dir).iterdir())))\n",
    "for inp in tqdm(inputs):\n",
    "    index, features_file = inp\n",
    "    print(index, features_file)\n",
    "     # Load\n",
    "    data_dict = torch.load(features_file, map_location='cpu')\n",
    "    print(data_dict.keys())   #['k', 'indices', 'file', 'id', 'model_name', 'patch_size', 'shape']\n",
    "    # print(\"shape=\", data_dict['shape'], \"k shape\", data_dict['k'].shape, \"patch_size=\", data_dict['patch_size'])\n",
    "    image_id = data_dict['file'][:-4]\n",
    "    print(image_id)\n",
    "    # Load\n",
    "    output_file = str(Path(output_dir) / f'{image_id}.pth')\n",
    "    if Path(output_file).is_file():\n",
    "        print(f'Skipping existing file {str(output_file)}')\n",
    "        # break\n",
    "        # return  # skip because already generated\n",
    "\n",
    "    # Load affinity matrix\n",
    "    feats = data_dict[which_features].squeeze().cuda()\n",
    "    # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "    if normalize:\n",
    "        feats = F.normalize(feats, p=2, dim=-1)\n",
    "    # print(\"After normalization, Features Shape\",feats.shape)\n",
    "    # print(\"which_matrix=\", which_matrix)\n",
    "    # Eigenvectors of affinity matrix\n",
    "    if which_matrix == 'affinity_torch':\n",
    "        W = feats @ feats.T\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "            # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = torch.eig(W, eigenvectors=True)\n",
    "        eigenvalues = eigenvalues.cpu()\n",
    "        eigenvectors = eigenvectors.cpu()\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity_svd':\n",
    "        USV = torch.linalg.svd(feats, full_matrices=False)\n",
    "        eigenvectors = USV[0][:, :K].T.to('cpu', non_blocking=True)\n",
    "        eigenvalues = USV[1][:K].to('cpu', non_blocking=True)\n",
    "        print(\"which matrix=\",which_matrix,\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of affinity matrix with scipy\n",
    "    elif which_matrix == 'affinity':\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "        W = (feats @ feats.T)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        if threshold_at_zero:\n",
    "            W = (W * (W > 0))\n",
    "        W = W.cpu().numpy()\n",
    "        # print(\"W shape=\", W.shape)\n",
    "        eigenvalues, eigenvectors = eigsh(W, which='LM', k=K)\n",
    "        eigenvectors = torch.flip(torch.from_numpy(eigenvectors), dims=(-1,)).T\n",
    "        print(\"which matrix=\",which_matrix, \"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "\n",
    "    # Eigenvectors of matting laplacian matrix\n",
    "    elif which_matrix in ['matting_laplacian', 'laplacian']:\n",
    "\n",
    "        # Get sizes\n",
    "        B, C, H, W, P, H_patch, W_patch, H_pad, W_pad = utils.get_image_sizes(data_dict)\n",
    "        if image_downsample_factor is None:\n",
    "            image_downsample_factor = P\n",
    "        H_pad_lr, W_pad_lr = H_pad // image_downsample_factor, W_pad // image_downsample_factor\n",
    "\n",
    "        # Upscale features to match the resolution\n",
    "        if (H_patch, W_patch) != (H_pad_lr, W_pad_lr):\n",
    "            feats = F.interpolate(\n",
    "                feats.T.reshape(1, -1, H_patch, W_patch),\n",
    "                size=(H_pad_lr, W_pad_lr), mode='bilinear', align_corners=False\n",
    "            ).reshape(-1, H_pad_lr * W_pad_lr).T\n",
    "\n",
    "        ### Feature affinities\n",
    "        # print(\"Without normalizing, Features Shape is\",feats.shape)\n",
    "\n",
    "        W_feat_ds = (feats @ feats.T)\n",
    "        # print(\"shape of w_feat_ds\", W_feat_ds.shape)\n",
    "        max_wfeatds=torch.max(W_feat_ds).item()\n",
    "        alpha=3\n",
    "        # print(\"Before Subtraction\")\n",
    "        count_positive = torch.count_nonzero(torch.greater_equal(W_feat_ds, 0.))\n",
    "        # print(\"positive values=\", count_positive)\n",
    "        W_feat_ds_sum = torch.sum(W_feat_ds)\n",
    "        # print(\"w_feat_ds_sum=\", W_feat_ds_sum)\n",
    "        W_feat_ds_mean = torch.mean(W_feat_ds)\n",
    "        # print(\"Mean of W_feat\", W_feat_ds_mean)\n",
    "        W_feat_ds_median = torch.median(W_feat_ds)\n",
    "        # print(\"Median of W_feat\", W_feat_ds_median)\n",
    "        W_feat_ds_std = torch.std(W_feat_ds)\n",
    "        # print(\"Standard Deviation of W_feat\", W_feat_ds_std)\n",
    "        W_feat_ds_max = torch.max(W_feat_ds)\n",
    "        # print(\"Maximum of W_feat\", W_feat_ds_max)\n",
    "        W_feat_ds_min = torch.min(W_feat_ds)\n",
    "        # print(\"Minimum of W_feat\", W_feat_ds_min)\n",
    "        # print(\"Factor value\", (max_wfeatds / alpha))\n",
    "\n",
    "        W_feat_ds = W_feat_ds - (max_wfeatds / alpha)\n",
    "\n",
    "        # print(\"After Subtraction\")\n",
    "        count_positive = torch.count_nonzero(torch.greater_equal(W_feat_ds, 0.))\n",
    "        # print(\"positive values=\", count_positive)\n",
    "        W_feat_ds_sum = torch.sum(W_feat_ds)\n",
    "        # print(\"w_feat_ds_sum=\", W_feat_ds_sum)\n",
    "        W_feat_ds_mean = torch.mean(W_feat_ds)\n",
    "        # print(\"Mean of W_feat\", W_feat_ds_mean)\n",
    "        W_feat_ds_median = torch.median(W_feat_ds)\n",
    "        # print(\"Median of W_feat\", W_feat_ds_median)\n",
    "        W_feat_ds_std = torch.std(W_feat_ds)\n",
    "        # print(\"Standard Deviation of W_feat\", W_feat_ds_std)\n",
    "        W_feat_ds_max = torch.max(W_feat_ds)\n",
    "        # print(\"Maximum of W_feat\", W_feat_ds_max)\n",
    "        W_feat_ds_min = torch.min(W_feat_ds)\n",
    "        # print(\"Minimum of W_feat\", W_feat_ds_min)\n",
    "        proj_layer=nn.Linear(pca_comp,pca_comp).cuda()\n",
    "        elu = nn.ELU()\n",
    "        proj_model = nn.Sequential(proj_layer, elu ).cuda()\n",
    "        pred_layer=nn.Linear(pca_comp,pca_comp).cuda()\n",
    "        x0=feats\n",
    "\n",
    "        x0_arr=x0.cpu()\n",
    "        scale = np.random.uniform(0.8, 1.2)  # Random scaling factor between 0.8 and 1.2\n",
    "        translation = np.random.uniform(-10, 10, size=2)  # Random translation vector between -10 and 10 in both directions\n",
    "        rotation = np.random.uniform(-15, 15)  # Random rotation angle between -15 and 15 degrees\n",
    "        shear = np.random.uniform(-0.2, 0.2, size=2)  # Random shear factor between -0.2 and 0.2 in both directions\n",
    "\n",
    "        # Define the affine matrix\n",
    "        affine_matrix = np.array([[scale * np.cos(rotation), -shear[0] * scale * np.sin(rotation), translation[0]],\n",
    "                                  [shear[1] * scale * np.sin(rotation), scale * np.cos(rotation), translation[1]],\n",
    "                                  [0, 0, 1]])\n",
    "        # print(x0_arr.shape)\n",
    "        x1_arr=affine_transform(x0_arr, affine_matrix)\n",
    "\n",
    "        z0_arr= pca.fit_transform(x0_arr)\n",
    "        z1_arr= pca.fit_transform(x1_arr)\n",
    "        # Define the affine transformation parameters\n",
    "\n",
    "        # z1_arr=affine_transform(z0_arr, affine_matrix)\n",
    "#         z1_arr=pca.fit_transform(z1_arr)\n",
    "        z0 = torch.from_numpy(z0_arr).float()\n",
    "        z1 = torch.from_numpy(z1_arr).float()\n",
    "\n",
    "        # feat_list.append(feats)\n",
    "        feat_dataset_z0 = Feature_Dataset(z0)\n",
    "        if feats.shape[0]%2==0:\n",
    "            features_dataloader_z0 = DataLoader(feat_dataset_z0, batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            features_dataloader_z0 = DataLoader(feat_dataset_z0, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        feat_dataset_z1 = Feature_Dataset(z1)\n",
    "        if feats.shape[0]%2==0:\n",
    "            features_dataloader_z1 = DataLoader(feat_dataset_z1, batch_size=batch_size, shuffle=True)\n",
    "        else:\n",
    "            features_dataloader_z1 = DataLoader(feat_dataset_z1, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "        criterion = NegativeCosineSimilarity()\n",
    "        proj_optimizer = torch.optim.SGD(proj_layer.parameters(), lr=0.06)\n",
    "        pred_optimizer = torch.optim.SGD(pred_layer.parameters(), lr=0.06)\n",
    "        print(\"Starting Training\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for z0_new,z1_new in zip(features_dataloader_z0,features_dataloader_z1):\n",
    "                z0_new = z0_new.to(device)\n",
    "                z1_new = z1_new.to(device)\n",
    "                # z0_new=proj_layer(z0_new)\n",
    "                z0_new=proj_model(z0_new)\n",
    "                # z1_new=proj_layer(z1_new)\n",
    "                # z1_new=proj_layer(z1_new)\n",
    "                z1_new=proj_model(z1_new)\n",
    "    #             print(\"z0_new.shape\", z0_new.shape)\n",
    "    #             print(\"z1_new.shape\", z1_new.shape)\n",
    "                p0=pred_layer(z0_new)\n",
    "                p1=pred_layer(z1_new)\n",
    "    #             print(\"p0.shape\", p0.shape)\n",
    "    #             print(\"p1.shape\", p1.shape)\n",
    "\n",
    "                z0_new=z0_new.detach()\n",
    "                z1_new=z1_new.detach()\n",
    "                loss = 0.5 * (criterion(z0_new, p1) + criterion(z1_new, p0))\n",
    "                total_loss += loss.detach()\n",
    "                loss.backward()\n",
    "                proj_optimizer.step()\n",
    "                pred_optimizer.step()\n",
    "                proj_optimizer.zero_grad()\n",
    "                pred_optimizer.zero_grad()\n",
    "            avg_loss = total_loss / len(features_dataloader_z0)\n",
    "            print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "        # z0_projected=proj_layer(z0.to(device))\n",
    "        projected_feature=pred_layer(z0.to(device))\n",
    "        print(projected_feature.shape)\n",
    "        W_feat_siam=torch.matmul(projected_feature, projected_feature.t())\n",
    "        # max_wfeatsiam=torch.max(W_feat_siam).item()\n",
    "        # alpha=3\n",
    "        # W_feat_siam = W_feat_siam - (max_wfeatsiam / alpha)\n",
    "        # W_feat_siam=torch.matmul(projected_feature[0], projected_feature[0].t())\n",
    "        W_feat=W_feat_ds + 0.1*W_feat_siam\n",
    "#         W_feat=normalize_affinity_matrix(W_feat_unnorm, axis=1)\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # print(\"W_feat.shape=\", W_feat.shape)\n",
    "        # W_feat=contrastive_affinity(feats, feats.T)\n",
    "        if threshold_at_zero:\n",
    "            W_feat = (W_feat * (W_feat > 0))\n",
    "        W_feat = W_feat / W_feat.max()  # NOTE: If features are normalized, this naturally does nothing\n",
    "        # W_feat = W_feat.cpu().numpy()\n",
    "        W_feat = W_feat.detach().cpu().numpy()\n",
    "        # print(\"W_feat shape=\",W_feat.shape)\n",
    "\n",
    "        ### Color affinities\n",
    "        # If we are fusing with color affinites, then load the image and compute\n",
    "        if image_color_lambda > 0:\n",
    "\n",
    "            # Load image\n",
    "            image_file = str(Path(images_root) / f'{image_id}.jpg')\n",
    "            image_lr = Image.open(image_file).resize((W_pad_lr, H_pad_lr), Image.BILINEAR)\n",
    "            image_lr = np.array(image_lr) / 255.\n",
    "\n",
    "            # Color affinities (of type scipy.sparse.csr_matrix)\n",
    "            if which_color_matrix == 'knn':\n",
    "                W_lr = utils.knn_affinity(image_lr / 255)\n",
    "            elif which_color_matrix == 'rw':\n",
    "                W_lr = utils.rw_affinity(image_lr / 255)\n",
    "\n",
    "            # Convert to dense numpy array\n",
    "            W_color = np.array(W_lr.todense().astype(np.float32))\n",
    "            # print(\"W_color shape\", W_color.shape)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # No color affinity\n",
    "            W_color = 0\n",
    "\n",
    "        # Combine\n",
    "        W_comb = W_feat + W_color * image_color_lambda  # combination\n",
    "        D_comb = np.array(utils.get_diagonal(W_comb).todense())  # is dense or sparse faster? not sure, should check\n",
    "        # print(\"W_comb shape= \", W_comb.shape, \"D_comb shape\",  D_comb.shape)\n",
    "        if lapnorm:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM', M=D_comb)\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM', M=D_comb)\n",
    "        else:\n",
    "            try:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, sigma=0, which='LM')\n",
    "            except:\n",
    "                eigenvalues, eigenvectors = eigsh(D_comb - W_comb, k=K, which='SM')\n",
    "        eigenvalues, eigenvectors = torch.from_numpy(eigenvalues), torch.from_numpy(eigenvectors.T).float()\n",
    "    print(\"eigenvalues shape\", eigenvalues.shape, \"eigenvectors shape\", eigenvectors.shape)\n",
    "    # Sign ambiguity\n",
    "    for k in range(eigenvectors.shape[0]):\n",
    "        if 0.5 < torch.mean((eigenvectors[k] > 0).float()).item() < 1.0:  # reverse segment\n",
    "            eigenvectors[k] = 0 - eigenvectors[k]\n",
    "\n",
    "    # Save dict\n",
    "    output_dict = {'eigenvalues': eigenvalues, 'eigenvectors': eigenvectors}\n",
    "    torch.save(output_dict, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}